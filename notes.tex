\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{mdframed}
\usepackage{lipsum}
\usepackage{blindtext}
\usepackage{geometry}
\usepackage{amssymb}


\geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\newmdtheoremenv{theorem}{TEOREMA}


\title{
    \textbf{Algebra e Geometria}\\
    \large A.A. 2022/2023
}
\date{}

\begin{document}

\author{Luca Marchi}
\begin{multicols*}{2}
\tableofcontents
\end{multicols*}


\maketitle

\section{Sezione prima}

\subsection{Insiemi}
Prodotto cartesiano: $ \boldsymbol{A \times B}: \{(a,b) = a \in A, b \in B\}$. \\
Una \textbf{relazione} é un sottoinsieme del prodotto cartesiano di due o più insiemi, quindi una relazione su $A$ é un sottoinsieme di $A \times A$ (scelgo alcune coppie di $A \times A$).\\
Scrivo $a_{1}Ra_{2}$ se $(a_{1}, a_{2})\in R$ e dico "$a_{1}$ é in relazione con $a_{2}$".\\\\
\underline{\textbf{Def}}. Una relazione é di \textbf{equivalenza} se rispetta le seguenti proprietà:

\begin{itemize}
    \item Riflessiva: se $aRa \forall a\in A$ (ogni elemento é in relazione con se stesso
    \item Simmetrica: se $a_{1}Ra_{2}$ allora $a_{2}Ra_{1}$
    \item Transitiva: se $a_{1}Ra_{2} e a_{2}Ra_{3}$ allora $a_{1}Ra_{3}$
\end{itemize}

\subsection{Funzioni o applicazioni}
$f : X \longrightarrow Y$

\begin{itemize}
    \item $f$ é \textbf{Iniettiva} se "elementi diversi vanno in elementi diversi", cioè $f(x_{1}) = f(x_{2}) \Rightarrow x_{1} = x_{2}$.
    \item $f$ é \textbf{Suriettiva} se "ogni elementi di $Y$ viene da qualche elemento di $X$", cioè se $\forall y\in Y,$ $\exists$ $ x\in X$ $/$ $ y = f(x)$.
    \item $f$ é \textbf{Biunivoca} se é sia \textbf{iniettiva} sia \textbf{suriettiva}, cioè ogni elemento di $Y$ viene da uno e un solo elemento di $X$, $\forall y \in Y, \;\exists! \; x \in A \;/\; y = f(x)$.\\
    $f^{-1}$ da $Y$ a $X$ é la funzione inversa.
\end{itemize}

\noindent Esempi:

\begin{itemize}
    \item[-] $f : \mathbb{R} \longrightarrow \mathbb{R}$, $\;f(x) = x^2$, non iniettiva, non suriettiva (es -2 non é $x^2$ per nessun $x \in \mathbb{R}$) 
    \item[-] $f: (0,+\infty) \longrightarrow \mathbb{R},\; f(x) = x^2$, é iniettiva, ma non suriettiva
    \item[-] $f: \mathbb{R} \longrightarrow (0,+\infty), \; f(x) = x^2$, é suriettiva ma non iniettiva
    \item[-] $f: (0,+\infty) \longrightarrow (0,+\infty), \; f(x) = x^2$, è sia suriettiva sia iniettiva quindi biunivoca. Esiste la funzione inversa e vale: $f^{-1}(y) = \sqrt{y}$
\end{itemize}

\subsection{Numeri}

\begin{itemize}
    \item[] \textbf{Naturali} : $\mathbb{N} = \{0,1,2,3,4...\}$, operazioni definite: $+,\cdot$, non sono presenti né gli opposti né gli inversi.
    \item[] \textbf{Interi} : $\mathbb{Z} = \{0,1,-1,2,-2,3,-3...\}$, operazioni definite: $+,\cdot,-$, non sono presenti gli inversi.
    \item[] \textbf{Razionali} : $\mathbb{Q} = \{\frac{p}{q}, p, q \in \mathbb{Z}, q \neq 0\} $, operazioni definite : $+,\cdot,-,:$, non sono presenti i limiti $(-\infty, +\infty)$.
    \item[] \textbf{Reali} : $\mathbb{R} = \{-\infty...\;0...+\infty\}$, non sono presenti le radici dei numeri negativi
    \item[] \textbf{Complessi} : $\mathbb{C} = \{a + bi, a\in{\mathbb{R}}, b\in{\mathbb{R}}\}, \;\; \boldsymbol{i^2 = -1}$, $i$ é chiamata $\rightarrow$ unità immaginaria.
\end{itemize}

\noindent\subsubsection{Campi}
\textbf{\underline{Def}}: $(\mathbb{K}, +, \cdot)$ é un \textbf{campo} se:

\begin{enumerate}
    \item valgono le proprietà commutativa, associativa e distributiva su $+, \cdot$.
    \item esistono elementi neutri, rispettivamente (0,1).
    \item ogni $x \in \mathbb{K}$ ha opposto $-x / x+(-x) = 0  $ ,ogni  $x\in \mathbb{K}, x \neq 0 $ ha inverso $x^{-1} / (x \cdot x^{-1} = 1)$
\end{enumerate}
$\mathbb{Q,R,C}$ sono campi $\mathbb{N,Z}$ no.

\subsubsection{Campi finiti}
Dato un numero intero $n \ge 0$, definiamo su $\mathbb{Z}$ la relazione di equivalenza
$$a \equiv b (n) \iff \exists k \in Z : a - b = k \cdot n$$
essa rispetta tutte e 3 le proprietà che definiscono un campo.
Definiamo $[b] = \{a \in Z : a \equiv b (n)\}$ e $Z_n = \{[0], [1], ..., [n-1]\}$.\\\\
Es. in $Z_2 = \{[0], [1]\}$, $[0]$ sono i numeri pari, $[1]$ quelli dispari. \\\\
Definiamo su $Z_n$ le operazioni:\\
$$[a] + [b] = [a+b], [a] \cdot [b] = [a \cdot b]$$ \\
Es. Possiamo scrivere, con la notazione dei campi finiti, il prodotto tra numeri interi: \\
Dato $Z_2$: $[0] \cdot [0] = [0], [0] \cdot [1] = [0 \cdot 1] = [0], [1] \cdot [1] = [1 \cdot 1] = [1]$. \\\\
$Z_n$ é un campo $\iff n$ é \textbf{primo}. Se $n$ non é primo, non esisterà l'inverso di un fattore di $n$, ovvero non esisterà nessuna classe di elementi che se moltiplicata con la classe del fattore restituisca classe 1.

\subsection{Spazio vettoriale}
\textbf{\underline{Def}}: uno \textbf{spazio vettoriale} definito su campo $\mathbb{K}$, é un insieme $V$ con due operazioni:
\begin{enumerate}
    \item[$\boldsymbol{+}$]: $V \times V \longrightarrow V \quad\quad (v_1,v_2) \rightarrow v_1 + v_2$
    \item[$\boldsymbol{\cdot}$]: $\underbrace{\mathbb{K}}_{costante}\times V \longrightarrow V \quad\quad (a,v) \rightarrow av$\\
\end{enumerate}

\noindent Gli elementi di $V$ sono detti \textbf{vettori}, gli elementi di $\mathbb{K}$ sono detti \textbf{scalari}.\\
$(+,\cdot)$ verificano le seguenti proprietà: $+$ é commutativa, associativa, con elemento neutro (vettore nullo) e opposti ($-v$), $\cdot$ é associativa, distributiva rispetto alla somma e con elemento neutro.\\\\
$v_1 + v_2 = v_2 + v_1 \quad \forall v_1,v_2 \in V$\\\\
$(v_1 + v_2)+v_3 = v_1+(v_2+v_3) \quad \forall v_1,v_2,v_3 \in V$\\\\
Esiste un "vettore nullo" $0 \in V \;/\; 0 + v = v \quad \forall v \in V$ \\\\
$\forall v \in  V \; \exists -v / v+(-v) = 0$\\\\
$a(bv) = (ab)v \quad\quad (a_1 + a_2)v = a_1v + a_2v, \quad a(v_1+v_2) = av_1 + av_2$\\

\noindent \textbf{Esempi di spazi vettoriali}:
\begin{enumerate}
    \item Somma vettoriale metodo del parallelogramma:\\
    \begin{figure}[h!]
        \centering
        \includegraphics[width = 5cm]{somma-vettoriale.png}
        \caption{Somma di vettori}
        
    \end{figure}
    
    \item $\forall \; \mathbb{K}, \mathbb{K}^{n}$ é uno spazio vettoriale $\mathbb{K}^{n} = \{(x_1...x_n), x_i \in \mathbb{K} \; \forall \;i=1...n\}$ su $\mathbb{K}$\\
    $v = (x_1...x_n) \quad u=(y_1...y_n)\quad a\in \mathbb{K}$\\
    $v+u = (x_1+y_1...x_n+y_n)\quad av=(ax_1...ax_n)$\\
    Quindi $\mathbb{R}^n$ é uno spazio vettoriale su $\mathbb{R}$, ${\mathbb{Z}_p}^{n}$  é uno spazio vettoriale su $\mathbb{Z}_p$ etc...
    \item I polinomi a coefficienti in $\mathbb{K}, \; \mathbb{K}[x] $ sono uno spazio vettoriale su $\mathbb{K}$ rispetto alle operazioni usuali.\\
    \item Se $X$ é un insieme $V =\{$ funzione $X \rightarrow \mathbb{K}\}$ é uno spazio vettoriale su $\mathbb{K}$ con le operazioni:
    \begin{itemize}
        \item $(f+g)(x) \;=\; f(x)+g(x) \quad \forall x \in X, \; \forall \;f,g \in \;V$
        \item $(a \cdot f)(x)\; = \; a \cdot f(x) \quad \forall a \in \; \mathbb{K}$
    \end{itemize}
\end{enumerate}
\subsection{Sottospazio vettoriale}
\textbf{\underline{Def}}: Un sottoinsieme non vuoto $U \subseteq{V}$ é un \textbf{sottospazio vettoriale} di $V$ se é chiuso rispetto a tali operazioni, cioè:
\begin{itemize}
    \item[] $\forall v_1, v_2 \in U \Longrightarrow v_1+v_2 \in U$
    \item[] $\forall v_1 \in U, \forall a \in \mathbb{K}, \; a\cdot v_1 \in U$
\end{itemize}
\textbf{Esempi}:
\begin{enumerate}
    \item $V = \mathbb{R}^2$ spazio vettoriale sul campo $\mathbb{R}$\\
    $U = \{(x,y) \in \mathbb{R}^2 \;/\; y = 2x\}$, é un SSV? (mi sto chiedendo se U sottoinsieme di V soddisfi le proprietà per essere un sottospazio vettoriale).\\
    se $v_1 = (x_1,y_1) \in U \Longrightarrow y_1 = 2x_1$\\
    se $v_2 = (x_1 + y_2) \in U \Longrightarrow y_2 = 2x_2$\\
    $v_1 +v_2 = (x_1 + x_2, y_1+y_2)$ e $y_1 + y_2 = 2(x_1 + x_2 \Longrightarrow v_1+v_2 \in U$(soddisfa l'operazione +) \\
    Inoltre $\forall a \in \mathbb{R} \quad a\cdot v_1 = (ax_1, ay_1)$ e $ay_1 = 2(ax_1) \Longrightarrow a\cdot v_1 \in U$ (soddisfa l'operazione $\cdot$) $\Longrightarrow U$ é un SSV.
    \item $V = \mathbb{R}^2$ spazio vettoriale sul campo $\mathbb{R}$\\
    $C = \{(x,y) \in \mathbb{R}^2 / x^2 + y^2 = 1\}$, é un SSV? No! Non lo é.\\
    Se prendo ad esempio $v_1 = (1,0) \;e \;v_2 = (0,1) \in C$, ma $v_1 + v_2 = (1,1) \notin C$ e $2v_1 \notin C$\\
    \item $V = \mathbb{R}^2$ spazio vettoriale sul campo $\mathbb{R}$\\
    $W = \{(x,Y) \in \mathbb{R}^2 / x,y \in \mathbb{Z}\}$, é un SSV?\\
    $\forall v_1, v_2 \in W, v_q+v_2 \in W$? Sì (soddisfa la prima operazione.\\
    $\forall v_1 \in W, \forall a \in \mathbb{R} \;a\cdot v_1 \in W$? No! (non soddisfa la seconda operazione), per esempio $\frac{1}{2}\cdot (1,0) = (\frac{1}{2}, 0) \notin W$\\
    \item $V = \mathbb{R}^2$ spazio vettoriale sul campo $\mathbb{R}$\\
    $S = \{(x,Y) \in \mathbb{R}^2 / \; xy \ge 0\}$, é un SSV?\\
    $\forall v \in S, \; a\in \mathbb{R} \; av \in S?$, Si (soddisfa la seconda operazione)\\
    $\forall v_1, v_2 \in S, \; v_1 + v_2 \in S?$ No! (non soddisfa la seconda operazione)\\
    $\Rightarrow S$ non é un SSV di $V$.
    \item $V = \mathbb{R}^3 = \{(x,y,z),x,y,z \in \mathbb{R}^3)$ spazio vettoriale sul campo $\mathbb{R}$\\
    $U = \{(x,y,z) \in V \;/\; z= 1 \} = \{(x,y,1),x,y \in \mathbb{R}\}$, é un SSV?\\
    $u_1 = (x_1,y_1,1), \; u_2 = (x_2,y_2,1) \quad u_1+u_2 = (x_1+x_2, y_1+y_2,2) \notin U$\\
    $\Longrightarrow U$ non é SSV.
    \item $V = \mathbb{R}^3 = \{(x,y,z),x,y,z \in \mathbb{R}^3)$ spazio vettoriale sul campo $\mathbb{R}$\\
    $W = \{(x,y,z) \in \;V /; z=0\} = \{(x,y,0),x,y\in \mathbb{R} $ é un SSV?\\
    se $w_1,w_2 \in W \Rightarrow w_1+w_2 = (x_1+x_2, y_1+y_2, 0)\in W$, (soddisfa la prima operazione)\\
    se $a\in \mathbb{R}, w\in W \Rightarrow aw \in W \Rightarrow W$ é un SSV (soddisfa anche la seconda condizione)
    \item $V = \mathbb{R}[x] = \{polinomi \; a \;coef.\; in\; \mathbb{R}\}$\\
    $U = \{polinomi\; di\; grado\; 2\}$, é un SSV?\\
    $u_1 = x^2 - 3x + 4, \; u_2 = -x^2 + x - 2\quad u_1+u_2 = -2x + 2$, (grado 1) $\Rightarrow \notin U$, (non soddisfa la prima operazione)\\
    $\Longrightarrow U$ non è SSV.
    \item $V = \{funzioni\; da\; \mathbb{R} \Longrightarrow \mathbb{R}\}$\\
    $U = \{funzioni\; continue\; da\; \mathbb{R} \Longrightarrow \mathbb{R}$, é un SSV? Sì\\
    La somma di funzioni continue sarà a sua volta una funzione continua quindi $\in U$, soddisfa sia la prima sia la seconda operazione.
\end{enumerate}
\underline{OSS}: $U$ é un SSV di $V \iff \forall u_1,u_2 \in U \; \forall a_1,a_2 \in \mathbb{K}, \; \underline{a_1u_1 + a_2+u_2 \in U}.$\\
\textbf{Dim}: $\Rightarrow$ se $U$ é un SSV e $u_1,u_2 \in U \Rightarrow a_1u_1, a_2u_2 \in U \; \forall a_1,a_2 \in \mathbb{K}$\\
$\Leftarrow$ se $a_1u_1 + a_2u_2 \in U \forall a_1,a_2 \in \mathbb{K}$, in particolare prendendo $a_1 = 1, a_2 = 1$, ho che $u_1+u_2 \in U$, e prendendo un qualsiasi $a_2 = 0$ ho che $a_1u_1 \in U$, cioè V è un SSV. \# \\\\
\textbf{\underline{Def}} : Dati $v_1, v_2...v_n \in V$, diciamo che $v\in V$ é una \textbf{combinazione lineare} di $v_1,v_2...v_n$ se esistono $a_1,a_2...a_n \in \mathbb{K}\; /\; v=a_1v_1+a_2+v_2...+a_nv_n$. Per quanto osservato nella pagina precedente, $U$ é un SSV $\iff$ contiene tutte le combinazioni lineari di tutti i propri elementi.\\\\
Esempi:
\begin{enumerate}
    \item $V = \mathbb{R}^2\quad v_1 = (1,0), v_2 = (0,1), v=(3,-2)$, v é comb. lin. di $v_1,v_2$?\\Sì 
    perchè $\exists a_1 = 3, a_2 = -2 \in \mathbb{R} \;/\; 3v_1 - 2v_2 = (3,-2) = v$\\
    \item $V = \mathbb{R}^2 \; u_1 = (2,0), u_2 = (-1,0), v=(3, -2)$, v é comb. lin. di $u_1,u_2$? No! 
    $a_1u_1 + a_2+u_2 = (2a_1,0) = (2a_1,0) + (-a_2,0) = (2a_1 - a_2,0) \neq v=(3,-2)$
\end{enumerate}

\noindent Diciamo che un SSV U di V é \textbf{generato} da $\{v_1...v_n\}$ se ogni $u\in U$ é comb. lin. di $v_1...v_n$, cioè se $\forall u \in U \; \exists a_1...a_n \in \mathbb{K} \;/\; u = a_nv_1+...a_nv_n)$. In questo caso diciamo anche che U é lo \textbf{span} di $v_1...v_n$ e scriviamo $U = <v_1...v_n>$\\\\
Esempio:
\begin{enumerate}
    \item $V = \mathbb{R}^4 = \{(x,y,z,w),x,y,z,w \in \mathbb{R}\}, \; v_1 = (2,0,0,0), v_2 = (0,1,-1,0)$\\
    Il SSV generato da $v_1, v_2$ é $<v_1,v_2> \;= \{a_1v_1+a_2v_2, a_1 a_2 \in \mathbb{R}\}= \\= \{(2a_1,0,0,0) + (0,a_2, -a_2,0), a_1,a_2 \in \mathbb{R}\} =\\= \{(2a_1,a_2,-a_2,0), a_1,a_2 \in \mathbb{R}\} = \{(x,y,z,w) \in \mathbb{R}^2\;/\; y+z = 0, w = 0\}$
    \item $V = \mathbb{R}[x]$, il SSV generato dallo span $<1,x,x^2> \; = \{a_01 + a_1x + a_2x^2\} = \{$ tutti i polinomi di grado $ \le 2\}$
    \item $V = \mathbb{R}^3 \quad v_1 = (1,0,0) , v_2 = (0,1,0)$\\
    $<v_1,v_2> = \{a_1v_1+a_2v_2, a_1,a_2 \in \mathbb{R} \} = \{(x,y,z) \in \mathbb{R}^3 \;/\; z = 0\}$\\
    $v_3 = (0,0,0)\; v_4 = (2,1,0) \; v_5 = (-1,3,0)$\\
    Lo span di questi 5 vettori resta uguale allo span $<v_1,v_2>$, quindi $v_3,v_4,v_5$ sono "inutili" perché aggiungendoli non ottengo nulla di nuovo (tutti e tre si possono ricavare da $v_1 e v_2$).\\
    Diremo che sono \textbf{linearmente dipendenti}.
\end{enumerate}
\textbf{\underline{Def}}: Un insieme di vettori $\{v_1, v_n\}$ é \textbf{linearmente indipendente}\\
1) se nessun vettore é comb. lin. degli altri.\\
2) cioè se l'unica comb. lin. di $v_1...v_n$ che dà 0 é quella $a_1 = 0... a_n = 0$, se $a_1v_1 + ... a_nv_n = 0 \Longrightarrow a_1 = 0... a_n = 0$.\\
Un insieme di vettori é \textbf{linearmente dipendente} se non é linearmente indipendente.\\\\
Perché le due definizioni sopra elencate sono equivalenti?:\\
Se 1) é falsa, c'è un $v_i$, supponiamo $v_1 \;/\; v_1 = a_2v_2 + a_n...v_n$.\\ Ma allora $0 = -v_1 + a_2v_2 + a_n...v_n$ cioè 2) é falsa.\\
Se invece 2) è falsa, cioè $\exists a_1...a_n$ con almeno un $a_i \neq 0 \;/\; a_1v_1 + a_n+v_n = 0$ allora $v_i = \frac{a_1}{a_i}v_1 + \frac{a_n}{a_i}v_n$ cioè 1) é falsa.\\\\
Esempio:\\
$V = \mathbb{R}^2\; v_1 = (-1,2) \; v_2 = (2,4)$\\
$\{v_1,v_2\}$ é lin. dipendente perché $v_2 = -2v_1$, cioè $2 \cdot v_1 + 1 \cdot v_2 = (0,0)$ e $(2,1) \neq (0,0)$.
\subsection{Base}
\textbf{\underline{Def}}: Sia $V$ uno spazio vettoriale su $\mathbb{K}$, diciamo che $\{v_1,...v_n\}$ é una \textbf{base} di $V$ se $\{v_1,...v_n\}$ é lin. indipendente e genera $V$.\\
Esempio:\\
$\begin{cases}
\{(1,1),(2,2)\} $ non é ind. e non genera V$\\
\{(1,0),(0,1)(1,1) $ non é ind. ma genera $\mathbb{R}^2\\
\{(1,0)\} $ é indip. ma non genera $ \mathbb{R}^2\\
\end{cases} \Longrightarrow $ non sono basi$ 
$\\
$\{(1,0), (0,1)\}$ é indip. e genera $\mathbb{R}^2$

Sia $V$ uno spaio vettoriale su campo $\mathbb{K}$ e sia $B = \{v_1...v_n\}$.\\\\
\textbf{\underline{Def}}:
\begin{enumerate}
    \item \textbf{$B$ genera $V$} se ogni elemento di $V(v\in V)$ é combinazione lineare di $v_1...v_n$, cioè $\forall v\in V \exists a_1...a_n \in \mathbb{K}\;/\; v = a_1\cdot v_1...a_n\cdot v_n$.
    \item $B$ è linearmente indipendente.
    \item $B$ é una base di $V$ se genera $V$ ed é linearmente indipendente.\\
\end{enumerate}

 
 
\begin{theorem} B è una base $\iff$ ogni $v \in V$ si scrive in modo unico come comb. lin. di $v_1...v_n$, cioè $\forall v \in V \exists! a_1...a_n \in \mathbb{K} \;/\; v = v_1a_1...v_na_n$.
\label{teorema basi}
\end{theorem}
\noindent \textbf{\underline{Def:}} in questo caso $a_1...a_n$ sono detti le coordinate di $V$ nella base $B$.\\\\
\textbf{Dim:} $\boldsymbol{\Longrightarrow} $ Sia $B$ una base e $V \in V$ poiché $B$ genera $V, \exists a_1...a_n\;/\; v=a_1v_1...a_nv_n$. Per mostrare l'unicità supponiamo che $\exists b_1...b_n \;/\; v=b_1v_1...b_nv_n.\\ 
\underbrace{v-v}_{=0} = a_1v_1...a_nv_n - (b_1v_1...b_nv_n) = \; (a_1-b_1)v_1+...+(a_n-b_n)v_n$.\\
Ho una comb. lin. che genera un vettore nullo, poichè $B$ é lin. indipendente questo implica che $a_1-b_1=0...a_b-b_n=0$, cioè $a_1 = b_1... a_n=b_n$.\\
$\boldsymbol{\Longleftarrow}$ se $\forall v\in V \exists! a_1...a_n \in \mathbb{K} \;/\; a_1v_1+a_nv_n = v$ allora $B$ genera $V$. Inoltre $0v_1+...+0v_n = 0$ é l'unica comb. lin. che dà 0: vettore nullo. Quindi $B$ é lin. indip. $\Rightarrow B$ è una base.\\\\
Esempi:
\begin{itemize}
    \item $X_1 = \{v_1 = (0,1)\}$ non é una base perché non genera $V = \mathbb{R}^3$ e infatti, dato $v_2 = (2,3)$, non esiste $a_1\in \mathbb{R} \;/\; a_1v_1=v_2$.
    \item $X_2 = \{v_1=(1,0), v_2 = (0,1), v_3=(1,1)\}$ non é una base perché non é lin. indipendente. Infatti $v = (2,3) = 2v_1 + 3v_2 + 0v_3 = 0v_1 + v_2 + 2v_3$ quindi $a_1,a_2,a_3$ non sono unici.
    \item Dimostro che $B = \{u_1 = (1,1), u_2 = (1,-1)\}$ é una base.\\
    $\forall v = (x,y) \exists! a_1,a_2 \;/\; \underbrace{a_1u_1 + a_2u_2}_{(a_1+a_2,a_1-a_2)} = (x,y)$\\
    $
    \begin{cases}
        a_1 + a_2 = x\quad 2a_1 = x+y\quad a_1 = \frac{x+y}{2}\\ 
        a_1 - a_2 = y\quad 2a_2 = x-y\quad a_2 = \frac{x-y}{2}\\
    \end{cases}$\\
    Quindi $\forall v=(x,y) \in \mathbb{R}^2 \exists! a_1,a_2 \;/\; v = a_1v_1+a_2v_2$ ad esempio $v = (2,3) = \frac{5}{2}a_1 - \frac{1}{2}a_2$.
\end{itemize}
\subsubsection{Basi canoniche}
Una base per lo spazio vettoriale $V = \mathbb{R}^n$ é: $e_1 = (1,0,0...0), e_2 = (0,1,0...0), e_n=(0,0,...1)$.\\ In effetti $v = (x_1,x_2...x_n) = x_1e_1+ x_2e_2 + ...x_ne_n$ quindi $\exists! \;x_1...x_n \Rightarrow \{e_1...e_n\}$ é una base, detta la \textbf{base canonica} di $\mathbb{R}^n$.\\\\
Sia $V = \mathbb{R}[x] = $ polinomi a coeff. in $\mathbb{R}$.\\
Una base $V = \{1,x,x^2,x^3...\}$(\underline{base canonica dei polinomi}) ad esempio, troviamo le coordinate di $p(x) = x^3-4x+\frac{1}{2}$ in questa base.\\
$p(x) = \frac{1}{2}1+(-4)x+0x^2+1x^3+0x^4...$\\
Sottospazio U={polinomi di grado $\leq 2$}\\
Base canonica: $\{1,x,x^2\}$ = $\{ax^2+bx+c\}$.
\subsubsection{Estrarre e completare}
\textbf{\underline{Def}}: sia X un insieme di vettori che genera V. \textbf{Estrarre} una base da X significa trovare un $B \subseteq{X}$ che sia una base di U.
Esempio:\\
$V = \mathbb{R}^2 \quad X = \{v_1 =(1,0)\; v_2=(2,0)\;v_3=(0,1)\;v_4=(0,0)\}$\\
Base estratta da $X \Rightarrow B_1=\{v_1,v_3\} \quad B_2=\{v_2,v_3\}$.\\\\
\textbf{\underline{Def}}: sia X un insieme lin.indip. in V, \textbf{completare} X ad uns base di V significa aggiungere ad X altri vettori in modo da ottenere una base.\\
Esempio:\\ $X = \{v_1 = (1,1,0), v_2 = (0,0,1)\} \subseteq \mathbb{R}^3$\\
Non genera V perchè $<x> = \{a_1v_1+a_2v_2\} = \{(a_1,a_1,a_2)\}$.\\
Sia $v_3 = (x,y,z)$ con $\boldsymbol{x \neq y}$ ad esempio $v_3 = (1,0,0)$, $\{v_1,v_2,v_3\}$ é una base di $\mathbb{R}^3$. Ho \textbf{completato} $\{v_1,v_2\}$ ad una base di $\mathbb{R}^3$.
\section{Sezione seconda}
\subsection{Dimensione}

 
 \begin{theorem} Tutte le basi di uno stesso spazio vettoriale $V$ su campo $\mathbb{K}$ hanno lo stesso numero di elementi.
 \label{dimensione basi}
 \end{theorem}
\textbf{\underline{Def}}: Tale numero é detto la \textbf{dimensione} di $V$.\\
Esempi:
\begin{itemize}
    \item $V = \mathbb{R}^3\; U \subseteq V, \; U = \{(x,y,z) \in V\;/\; x-y = 0\}$\\
    Una base di U é data da $u_1 = (1,1,0), u_2 = (0,0,1)$ infatti $\{u_1,u_2\}$ é lin. indip. e $<u_1,u_2> = \{a_1u_1+a_2+u_2\}=\{a_1,a_1,a_2\}$, quindi $u_1,u_2$ generano U.\\
    Qualunque altra base di U ha 2 elementi $\Longrightarrow$ dim U = 2.
    \item $W = \{(x,y,z) \in V \;/\; x-y = 0, x=3z\}$\\
    $\{w_1 = (3,3,1)\}$ é una base di W $\Longrightarrow$ dim W = 1\\
    $<w_1> = \{a_1w_1, a_1 \in \mathbb R\} = \{3a,3a,a\} = W$.
\end{itemize}
Sia V uno spazio vettoriale di \textbf{dimensione d} su campo $\mathbb{K}$.
\begin{enumerate}
    \item ogni base di V contiene d elementi.
    \item ogni insieme linearmente indipendente di V contiene k elementi, $k\leq d$; può essere completato ad una base aggiungendo d-k elementi in modo opportuno.
    \item ogni insieme che genera V contiene g elementi, $g \geq d$, possiamo estrarre una base scegliendo in modo opportuno d vettori tra i g dell'insieme. \\NON VALE IL VICEVERSA (ogni insieme che contiene g elementi non genera V).
\end{enumerate}
\subsection{Forma cartesiana e parametrica}
Sia U un sottospazio di dimensione d di uno spazio vett. di dimensione n ($d \leq n$).\\
Posso esprimere U in 2 modi:\\
1) \textbf{In forma cartesiana}: U é identificato in $\mathbb{R}^n$ da n-d equazioni tra loro lin. indip.\\ (\underline{ogni equazione toglie un grado di libertà}).\\
2) \textbf{In forma parametrica}: U é espresso in funzione di d parametri.\\\\
Esempio:\\
In $v=\mathbb{R}^4,\; U = \{(x,y,z,w) \in \mathbb{R}^4 \;/\; \underbrace{x=2y, y=3z, w=0}_{equazioni\; cartesiane}\}$\\ dim U = 4(dimensione di V) - 3(eq. cartesiane) = 1.\\
Un altro modo per scrivere U é: $U=\{(6t,3t,t,0)\;t\in \mathbb R\} \Rightarrow$ 1 parametro (dim U = 1).\\\\
\underline{Come si passa da cartesiana a parametrica?}\\
Basta usare le equazioni cartesiane per esplicitare n-d coordinate in funzione delle altre. Le rimanenti sono i parametri.\\
Esempio:\\
$\{(x,y,z)\} \in \mathbb{R}^3\;/\; x+2y-z=0\}$ mi permette di scrivere $z=x+2y$.\\
Quindi se $x=t,\;y=s,\;z=t+2s\quad \{(t,s,t+2s),t,s\in \mathbb{R}\}$.\\\\
\underline{Come si passa da parametriche a cartesiane?}\\
Basta risolvere un sistema: $\{(t,-2t,s,t+2s)\} \in \mathbb{R}^4$:\\
$
\begin{cases}
 x=t\quad 2x+y=0\\
 y=-2t\quad x=t\\
 z=s\quad z=s\\
w=t+2s\quad x+2z-w=0
\end{cases}\\\\\\
$
\underline{OSS}: per \textbf{intersecare} i sottospazi é più comoda la forma cartesiana. Invece per \textbf{trovare una base} di un sottospazio é più comoda la forma parametrica.\\\\
Esempio:\\
$U=\{(t,-2t,s,t+2s)\}=\{(t,-2t,0,t)+(0,0,s,2s)\}=\{t\underbrace{(1,-2,0,1)}_{v_1}+s\underbrace{(0,0,1,2)}_{v_2}\}$\\
$=\{tv_1+sv_2\}=<v_1,v_2> \Longrightarrow$ $v_1,v_2$ sono una base di U.\\\\
\textbf{Prop}: L'intersezione di sottospazi vettoriali é un sottospazio vettoriale.\\
\textbf{Dim}: Sia V uno spazio vettoriale su $\mathbb{K}$ e siano U,W sottospazi.\\
$U\cap W = \{v\in V \;/\; v \in U, v\in W\}$, vogliamo mostrare che: se $v_1,v_2 \in U\cap W, v_1+v_2 \in U\cap W$\\
In effetti dato che U e W sono sottospazi vettoriali e che $v_1,v_2 \in U, v_1,v_2\in W$:\\
$\Longrightarrow v_1+v_2 \in U,\; v_1+v_2\in W$\\
$\Longrightarrow v_1+v_" \in U\cap W$\\
Analogamente se $v_\in U\cap W$ e $a\in \mathbb{K}, av \in U\cap W$, con lo stesso ragionamento.  \# \\\\
\underline{OSS}: Invece l'unione di sottospazi \underline{non} é un sottospazio! Facciamo un es:\\
$U = \{(x,y) \in \mathbb{R}^2 \;/\; x=0\} = \{(0,t), t\in \mathbb{R}\}$\\
$W=\{(x,y) \in \mathbb{R}^2 \;/\; y=0\} = \{(s,0), s\in \mathbb{R} \}$, $e_1 = (1,0)\in W, e_2=(0,1) \in U$\\
$\Longrightarrow e_1,e_2 \in U\cup W$ ma, $e_1+e_2 = (1,1) \notin U, \notin W$, e quindi $\notin U\cup W$.
\begin{figure}[h!]
        \centering
        \includegraphics[width = 10cm]{esempio_unione.png}
        \caption{Unione di sottospazi}
    \end{figure}\\
Il vettore $u$ nell'immagine rappresenta $e_1 + e_2$ e non appartiene a nessuno dei due insiemi.\\\\
Abbiamo visto che dato uno spazio vettoriale $V$ su campo $\mathbb{K}$, e dati due sottospazi U,W di V:
\begin{itemize}
    \item \textbf{$U\cap W$ é un sottospazio.}
    \item \textbf{$U\cup W$ non é un sottospazio.}
\end{itemize}
\subsection{Somma di sottospazi}
\textbf{\underline{Def}}: La somma di sottospazi U,W é:\\
$$\boldsymbol{\underbrace{U+W}_{\text{somma di sottospazi}} = \{\underbrace{u+w}_{\text{somma di vettori}},\; u\in U,w\in W\}}$$\\
Si verifica immediatamente che $U+W$ é un sottospazio di V:\\
- $(U+W)+(U'+W')=(\underbrace{u+u'}_{\in U}) + (\underbrace{w+w'}_{\in W})$\\
- $\forall a \in \mathbb{K} \; a(u+w) = au+aw \in U+W$\\\\
Esempio: $V = \mathbb{R}^3$, $U=\{(x,y,z) \in \mathbb{R}^3 \;/\; x+y=0, z=0\}=\{(t,-t,0),t\in \mathbb{R}\}$\\
$W=\{(x,y,z) \in \mathbb{R}^3 \;/\; x=y=0\}=\{(0,0,s),s\in \mathbb{R}\}$\\
$U+W = \{u+w,u\in U, w\in W\} = \{(t,-t,0) + (0,0,s), t\in \mathbb{R}, s\in \mathbb{R}\} =\{(t,-t,s), t,s\in \mathbb{R}\}=\{(x,y,z) \in \mathbb{R}^3 \;/\; x+y=0\} $\\\\
\underline{OSS}: dim U = 1, dim W = 1, dim U+W = 2 $\longrightarrow$ le rette si intersecano in un punto\\
Geometricamente U+W é l'unico piano di $\mathbb{R}^3$ che contiene le rette U,W.\\
OSS: $U\cap W = \{\underline{0}\}$ ha per base $\O \Rightarrow$ dim $U\cap W=0$\\
$\begin{cases}
    x=0\\
    y=0\quad \Longrightarrow(x,y,z) = (0,0,0) = \underline{0}\\
    z=0\\
    x+y=0\\
\end{cases}
$\\\\
Uno spazio vettoriale che contiene sono $\underline{0}$ si chiama sottospazio banale ed é $\neq$ da $\O$.
\begin{theorem}[formula di Grassman]
    
Siano U,W sottospazi di V.\\
\textbf{dim(U+W) = dim U + dim W - dim $\boldsymbol{U\cap W}$}
\label{formula di Grassman}
\end{theorem}

\noindent Idea di dim: Scelgo una base $\{v_1...v_k\}$ per $U\cap W$ e la completo ad una base $\{v_1...v_k,u_1...u_l\}$ di U e ad una base $\{v_1...v_k,w_1...w_m\}$ di W.\\
Con qualche caolcolo si può dimostrare che $\{v_1...v_k,u_1...u_l,w_1...w_m\}$ é una base di U+W. Da questo segue la formula. $\#$\\\\
Esempio: $V=\mathbb{R}^4$\\
$U=\{(x_1,x_2,x_3,x_4)\in V\;/\; x_1-x_4 = 0,x_2-x_3 = 0\}$\\ $W=\{(x_1,x_2,x_3,x_4)\in V \;/\; x_1-x_2 = 0, x_3-x_4 = 0\}$\\
Voglio trovare U+W.\\
$U\cap W = \{(x_1,x_2,x_3,x_4) \in V \;/\; x_1 = x_2, x_2=x_3, x_3 = x_4\} = \{(t,t,t,t), t\in \mathbb{R}$\\
$U\cap W$ ha dimensione 1 (compare solo un parametro nella forma parametrica), una base può essere $\{v_1=(1,1,1,1)\}$.\\
Formula di Grassman: dim U+W = dim U + dim W - dim $U\cap W$ = 2+2-1=3\\
$U =\{(t,s,s,t)\}$, $W=\{(a,a,b,b)\} \Longrightarrow U+W = \{(t+a,s+a,s+b,t+b)\} = $\\
$=\{(x_1,x_2,x_3,x_4)\in V\;/\; x_1-x_2=x_4-x_3\}$
\subsubsection{Somma diretta}
\textbf{\underline{Def}}: Siano U,W sottospazi di V. Diciamo che U,W formano \textbf{somma diretta} se $U\cap W = \{\underline{0}\}$. se U,W sono in somma diretta, scriviamo $U\oplus W$ al posto di U+W.\\\\
\textbf{Prop}: Sia V uno spazio vettoriale e siano U,W due sottospazi / $V=U\oplus W$.\\ Allora ogni vettore $v\in V$ si scrive in modo unico come $v = u+w$ con $u\in U, w\in W$.\\
\textbf{Dim}: $V=U\oplus W$("V é somma diretta di U e W") significa che 1) \textbf{V = U+W} e 2) $\boldsymbol{U \cap W = \{\underline{0}\}}$.\\ Poiché V = U+W , $\forall v \in V \exists u\in U , w\in W \;/\; v = u+w$.\\
Supponiamo che tale scrittura non sia unica, $\exists u' \in U, w' \in W \;/\; v = u+w=u'+w' \Longrightarrow \underbrace{u-u'}_{\in U}=\underbrace{w'-w}_{\in W}\in U\cap W$ ma $U\cap W = \{\underline{0}\}$,\\ cioè le due componenti di $v$ coincidono. \#\\
\underline{OSS}: Se V = U+W, ma $U\cap W \neq \{\underline{0}\}$ ci sono infiniti modi per scrivere $v\in V$ come $u+w$, con $u \in U, w\in W$.
\section{Sezione terza}
\subsection{Applicazione lineare}
\textbf{\underline{Def}}: Siano V,U due sottospazi vettoriali su campo $\mathbb{K}$.\\
Un'applicazine $f:V\longrightarrow U$ é \textbf{lineare} se "é compatibile con le operazioni di V e U", cioè se:
\begin{enumerate}
    \item $\forall v_1,v_2 \in V, \;\boldsymbol{f(v_1+v_2)=f(v_1)+f(v_2)}$
    \item $\forall v \in V, \forall a\in \mathbb{K},\; \boldsymbol{f(a\cdot v)=a\cdot f(v)}$
\end{enumerate}
In maniera equivalente, l'applicazione $f$ é lineare se e solo se $\forall v_1,v_2 \in V$ e $\forall a_1,a_2 \in \mathbb{K} f(a_1v_1 + a_2v_2) = a_1f(v_1)+a_2f(v_2)$\\\\
Esempio: $f: \mathbb{R}^3 \longrightarrow \mathbb{R}^2\quad f(x,y,z)=(x+y,y-z)$ é lineare?\\
1) Dati $v_1 = (x_1,y_1,z_1), v_2=(x_2,y_2,z_2)\quad v_1+v_2 = (x_1+x_2,y_1+y_2,z_1+z_2)$\\
$f(v_1+v_2) = (x_1+x_2+y_1+y_2, y_1+y_2 - z_1 - z_2)$\\
$f(v_1) + f(v_2) = (x_1+y_1, y_1-z_1) + (x_2+y_2, y_2-z_2) = (x_1+x_2+y_1+y_2, y_2+y_1-z_1-z_2)$\\
2)$f(av) = f(ax,ay,az) = (ax+ay,ay-az) = a(x+y,y-z) = af(v)$\\$ \Longrightarrow f$ é lineare
\subsection{Nucleo e Immagine}
Sia $f: V\longrightarrow U$ un'applicazione lineare:\\
\textbf{\underline{Def:}}
\begin{itemize}
\item \textbf{nucleo} di $f$ \underline{Kerf} = $\{v\in V \;/\; f(v) = \{\underline{0}\}\}$
\item \textbf{immagine} di $f$ \underline {Imf}=$\{f(v),v\in V\} = \{u\in U \;/\; \exists v\in V \;/\; f(v) = u\}$\\
\end{itemize}
\begin{figure}[h!]
        \centering
        \includegraphics[width = 6cm]{white.jpeg}
        \caption{nucleo e immagine}
    \end{figure}
\textbf{Prop}:
\begin{enumerate}
    \item \textbf{Kerf} é un sottospazio vettoriale di V
    \item \textbf{Imf} é un sottospazio vettoriale di U
\end{enumerate}
\textbf{Dim}:\\
\underline{Dimostro 1)} $\Rightarrow$ Dobbiamo mostrare che dati $v_1,v_2 \in$ Kerf anche $v_1+v_2 \in $ Kerf.\\
$f(v_1+v_2) \underbrace{=}_{\text{f é lin}} f(v_1) + f(v_2) = \underline{0}+\underline{0} = \underline{0}\quad \; f(v_1) = \underline{0}, f(v_2) = \underline{0}$\\ 
Analogamente, dato $a \in \mathbb{K}$ e $v\in$ Kerf (cioè $f(v)=\underline{0}$, ho che:\\
$f(av) = af(v) = a\underline{0} = \underline{0}\Longrightarrow av\in $Kerf.\\
Dunque ho dimostrato che Kerf é un SSV di V.\\
\underline{Dimostro 2)} $\Rightarrow$ Siano $u_1,u_2 \in $Imf, dunque esistono $v_1,v_2\in V\;/\; f(v_1) = u_1,\;f(v_2) = u_2$. \\Allora $f(v_1+v_2) = f(v_1)+f(v_2) = u_1+u_2$, cioè esiste un elemento di V che viene mandato in $u_1+u_2 \Longrightarrow u_1+u_2 \in$ Imf.\\
Analogamente, se $a\in \mathbb{K}$ e $u\in$Imf (cioè $\exists v\in V \;/\; f(v) = u$) allora $f(av) = af(v) = au$, ovvero esiste un vettore $av \in V \;/\; f(av) = au\Longrightarrow au \in $Imf.\\
Dunque ho dimostrato che Imf é un SSV di U.\\\\
Ricordiamo che $f:V \longrightarrow U$ é \textbf{suriettiva} $\iff$ ogni $u\in U$ è $u=f(v)$ per qualche $v\in V$ cioé se ogni $u\in U$ appartiene a Imf. Quindi:\\
\textbf{Prop}: $f$ é suriettiva $\iff$ \textbf{Imf = U} \# \\\\
Ricoridamo anche che $f:V\longrightarrow U$ é \textbf{iniettiva} se $v_1\neq v_2 \Rightarrow f(v_1) \neq f(v_2)$\\
\textbf{Prop}: $f$ é iniettiva $\iff$ Kerf = $\{\underline{0}\}$\\
\textbf{Dim}:\\
$\Rightarrow$ Poichè f é lineare $f(\underline{0}) = f(v-v) = f(v) -f(v) = \underline{0}$ cioè $\underline{0}\in$ Kerf. Poichè f é iniettiva, se $v\neq \underline{0}, f(v)\neq f(\underline{0})=\underline{0}$ quindi Kerf = $\underline{0}$.\\
$\Leftarrow$ Siano $v_1,v_2\in V \;/\; f(v_1) = f(v_2)$. Allora $\underline{0} = f(v_1)-f(v_2) = f(v_1-v_2)$ cioè $v_1-v_2 \in $Kerf, ma Kerf$=\{\underline{0}\}\Longrightarrow v_1-v_2 = \underline{0}$, cioè $v_1=v_2 \Rightarrow$ f é iniettiva. \# \\\\
Esempio:\\
$f:\mathbb{R}^2\longrightarrow \mathbb{R}^2\quad f(x,y)=(x-y, 2x-2y)$\\
Abbiamo visto che Kerf=$\{(t,t), t\in \mathbb{R}\} \neq \{\underline{0}\} \Rightarrow$ f non é iniettiva.\\
Imf = $\{(t,2t), t\in \mathbb{R}\} \neq \mathbb{R}^2 \Rightarrow$ f non é suriettiva.\\

\begin{theorem}[Teorema del rango]
Sia $f:V \longrightarrow U$ un'applicazione lineare.
Allora \textbf{dim V = dim Kerf + dim Imf}
\label{teorema del rango}
\end{theorem}
\textbf{Dim}: sia $v_1...v_k$ una base di Kerf. Completiamola ad una base di V, $v_1...v_k, v_{k+1}, v_n$.\\
Quindi per ogni $v\in V$, esistono e sono unici $a_1...a_n \;/\; v = a_1v_1+...a_kv_k+a_{k+1}v_{k+1}+...a_nv_n$.\\
Poichè f é lineare 
$f(v) = a_1\underbrace{f(v_1)}_{\underline{0}}+...a_k\underbrace{f(v_k)}_{\underline 0}+a_{k+1}v_{k+1}+...a_nf(v_n) = $\\
$=a_{k+1}f(v_{k+1})+...a_nf(v_n)$, cioè ogni vettore $f(v)$ di Imf si scrive in modo unico come cob lineare di $f(v_{kn})...f(v_n)$, cioè $f(v_{k+1})...f(v_n)$ é una base di Imf.\\ Quindi \underline{dim Kerf = k, dim V = n, dim Imf = n-k}.  \#\\\\
\textbf{Corollario}: Sia $f:V\longrightarrow U$ un'appl. lineare.
\begin{enumerate}
    \item Se dim V $>$ dim U, f non può essere iniettiva.
    \item Se dim V $<$ dim U, f non può essere suriettiva.
\end{enumerate}
\textbf{Dim}: 1)dim Imf $\leq$ dim U $<$ dim V $\overbrace{=}^{Teor. Rango}$ dim Imf + dim Kerf\\
cioè dim Kerf $> 0 \Rightarrow$  Kerf $\neq \{\underline 0\} \Rightarrow$ f non é iniettiva.\\
\textbf{Dim}: 2)dim Imf $\leq$ dim Imf + dim Kerf $\overbrace{=}^{Teor.Rango}$ dim V $<$ dim U\\
cioè dim Imf $<$ dim U quindi Imf$\neq U \Rightarrow$ f non é suriettiva. \# \\\\
\textbf{Corollario}: Sia $f:V\longrightarrow U$, un'appl. lineare e dim V = dim U.\\
Allora f é iniettiva $\iff$ f é suriettiva ($\iff$ é biunivoca)\\
\textbf{Dim}: dim Kerf + dim Imf = $\overbrace{=}^{Teor.Rango}$ dim V = dim U.\\
Quindi f é iniettiva $\iff$ dim Kerf = 0 $\iff$ dim Imf = U $\iff$ f é suriettiva. \#
\subsection{Isomorfismo}
\textbf{\underline{Def}}: $f:V \longrightarrow U$ é un \textbf{isomorfo} se é lineare e biunivoca.\\
Diciamo che V,U sono isomorfi se $\exists$ un isomorfo $V\longrightarrow U$. Essere isomorfi é una relazione di equivalenza.\\\\
Esempio:\\
$V  =\{p(x) \in \mathbb{R}[x]\;/\; p(x) \leq 2\} = \{ax^2+bx+c,a,b,c \in \mathbb{R}\}$, $U=\mathbb{R}^3$\\
$f:V\longrightarrow U$ é lineare ed é biunivoca $ax^2 + bx + c \rightarrow (a,b,c) \Rightarrow$ f é un isomorfo.\\
V e U sono isomorfi.
\begin{theorem}
    
Siano V,U due spazi vettoriali su $\mathbb{K}$ sia $\{v_1...v_n\}$ una base di V e siano $u_1...u_n \in U$. Allora $\exists$ una e una sola applicazione lineare $f:V\longrightarrow U\;/\; f(v_1) = u_1,...f(v_n)=u_n$.
\label{appl lineare unica}
\end{theorem}

\noindent\textbf{Dim}: sia $v\in V$. Poichè $\{v_1...v_n\}$ é una base, esistono e sono unici $a_1...a_n \in \mathbb \;/\; v = a_1v_1+...a_nv_n$.\\
Poiché f è lineare, $f(v) = a_1f(v_1)+...+a_nf(v_n) = a_1u_1+...+a_nu_n.$\\
In questo modo f é lineare ed é unicamente determinata, perchè ogni $v \in V$ può andare in un unico vettore di U. $\#$\\\\
Sia $f:V\longrightarrow U$ un'applicazione lineare e siano $B_v = \{v_1...v_n\}$ una base di V e $B_u = \{u_1...u_n\}$ una base di U.
\subsection{Matrici}
La \textbf{matrice} di f nelle basi $B_v, B_u$ é la tabella di numeri A che ha all'i-esima riga e j-esima colonna il coefficiente $a_{ij}$ che appare nell'espressione $f(v_j)=a_{1j}u_1+...+a_{nj}u_n$.\\
Infatti, poiché $f(v_j) \in U$ e $B_u$ una base, tali coefficienti $\exists!$. E poichè $B_v$ é una base, per il teorema precedente conoscere tutti questi coefficienti (cioè conoscere la matrice A) determina unicamente f.\\
A é la matrice che ha per j-esima colonna i coefficienti di $f(v_j)$ nella base $B_u$.\\\\
Esempio: $V=\mathbb{R}^3$, $U=\mathbb{R}^2$\\
$f:V\longrightarrow U\quad$ $f(x,y,z) = (x-y,y+2z)$\\
$B_v=\{v_1=(1,1,0), v_2 = (0,0,1), v_3 = (1,0,1)\}$\\ $B_u=\{u_1=(2,0), u_2=(0,-1)\}$\\
$f(v_1) = (0,1) = 0u_1+(-1)u_2 \quad \quad \quad \quad$ La matrice A di f nelle basi $B_v, B_u$\\
$f(v_2)=(-1,3)=-\frac{1}{2}u_1 -3u_2\quad \quad \quad \quad \quad \quad \quad$    
$ A=\begin{pmatrix}
0 & -\frac{1}{2} & \frac{1}{2}\\
-1 & -3 & -2
\end{pmatrix}$\\ 
$f(v_3)=(1,2)=\frac{1}{2}-2u_2$\\\\
\underline{OSS}: in basi diverse la stessa f ha matrice diversa!\\\\
Se $f,g:V\longrightarrow U$ sono appl. lineari, anche f+g(definita come$(f+g)(v) = f(v)+g(v) \forall v\in V)$ é lineare. E per ogni $a\in \mathbb{K}$, af(definita come $(af)(v)=af(v) \forall v\ in V$ é lineare.\\
Quindi l'insieme di tutte le applicazioni lineari:
$$Hom(V,U)=\{\text{appl. lineari } V\longrightarrow U\}$$
é esso stesso uno \underline{spazio vettoriale}.\\
Fissate basi di V,U se A é la matrice di f e B é la matrice di g, la matrice di f+g é A+B ottenuta sommando gli elementi corrispondenti. E la matrice di di af, sarà aA, ottenuta moltiplicando per a ogni elemento di A.\\
Rispetto a queste operazioni, l'insieme: 
$$M_{m,n} = \{\text{matrici mxn, con m righe e n colonne\}}$$
é esso stesso uno spazio vettoriale ed ho un isomorfo di spazi vettoriali.\\
$$\boldsymbol{Hom(V,U) \longrightarrow M_{m,n}}$$
f$\longrightarrow$ matrice f(che dipende dalla scelta delle basi) nelle basi $B_v, B_u$.\\\\
Dati sp. vettoriali V,U,W e appl. lineari f,g:    $\quad \quad V\underbrace{\overbrace{\longrightarrow}^{g} U \overbrace{\longrightarrow}^{f}}_{f\circ g} W$\\
posso considerare la composizione $f\circ g:v \longrightarrow f(g(v))$.\\\\
\textbf{Prop}: se f,g sono lineari lo é anche $f \circ g$.\\
\textbf{Dim}: $\forall a_1, a_2 \in \mathbb{K}\; \forall v_1,v_2 \in V (f\circ g)(a_1v_1+a_2v_2) = f(g(a_1v_1+a_2v_2)) \overbrace{=}^{\text{g lin}}$\\
$=f(a_1g(v_1) + a_2g(v_2)) \overbrace{=}^{\text{f lin}} a_1f(g(v_1)) + a_2f(g(v_2)) = a_1(f\circ g)(v_1) + a_2(f\circ g)(v_2)$ \#
\subsubsection{Prodotto tra matrici}
Sia $\{v_1...v_n\}$ una base di V, sia $\{u_1...u_m\}$ una base di U, sia $\{w_1...w_l\}$ una base di W.\\
$V\underbrace{\overbrace{\longrightarrow}^{g} U \overbrace{\longrightarrow}^{f}}_{f\circ g} W$, se A é la matrice di f e B é la matrice di g, chi é la matrice di $f\circ g$? Calcoliamola:\\
sia $j\in \{i...n\}$, $(f\circ g)(v_j) = f(\displaystyle\sum_{i=1}^{m} b_{ij}u_i)  \overbrace{=}^{\text{f é lin}} \displaystyle\sum_{i=1}^n b_{ij}f(u_i)  \overbrace{=}^{\text{def di A}}$
$\displaystyle\sum_{i=1}^{m}(b_{ij}\sum_{k=1}^{l}a_{ki}w_{k})$\\
Il \textbf{coefficiente} di $w_k$ é $\boldsymbol{\displaystyle\sum_{i=1}^{m} b_{ij}a_{ki}}$. La matrice di $f\circ g$ dovrà avere questo coefficiente alla k-esima riga e j-esima colonna.\\
Indichiamo questa matrice con \textbf{AB} e la chiamiamo il \textbf{prodotto riga per colonna di A e B}.
\begin{figure}[h!]
    \centering
    \includegraphics[scale = 0.3]{prodotto-tra-matrici.png}
    \caption{Prodotto tra matrici}

\end{figure}\\
\\\\\\\\\\\\
\subsection{Ancora isomorfismo...}


\begin{theorem}
    
Un'applicazione lineare é un isomorfismo $\iff$ manda basi in basi.
\label{app lineare e isomorfismo (basi in basi)}
\end{theorem}

\noindent\textbf{Dim}:\\
$\Rightarrow$ Sia $f:V\longrightarrow U$ un isomorfismo e sia $\{v_1...v_n\}$ una base di V. Vogliamo mostrare che $\{f(v_1)...f(v_n)\}$ é una base di U, cioè che ogni $u\in U$ si scrive in odo unico come comb. lin di $f(v_1)...f(v_n)$. \\Poiché f é biunivoca, $\exists! v\in V \;/\; f(v) = u$. Poiché $\{v_1...v_n\}$ é una base di V, $\exists! a_1...a_n \in \mathbb{K} \;/\; v=a_1v_1+...+a_nv_n$ e poiché f é lineare, $u = f(v) = a_1f(v_1) + ...+ a_n(fv_n)$, cioé la tesi.\\
$\Leftarrow$ Sia $\{v_1...v_n\}$ una base di V, e sia $f:V\longrightarrow U$ un'appl. lineare tale che $\{f(v_1)...f(v_n)\}$ é una base di U. Vogliamo dimostrare che f é biunivoca.\\
Sia $u\in U $. Poiché $\{f(v_1)...f(v_n)\}$ é una base $\exists! a_1...a_n \in \mathbb{K} \;/\; u = a_1f(v_1)+...+a_nf(v_n) = f(a_1v_1+...+a_nv_n)$, cioé $\forall u \in U \exists! v\in V (v = a_1v_1+...+a_nv_n)$ tale che $f(v) = u$ quindi f é biuivoca.\\\\
\textbf{Corollario}: L'inverso di un isomorfismo é un isomorfismo.\\
\textbf{Dim}: Sia $f:V\longrightarrow U$ un isomorfismo e sia $f^{-1}: U\longrightarrow V$ l'applicazione inversa, cioé $f^{-1}(u)=$ l'unico $v\in V \;/\; f(v) = u$. Sia $\{v_1...v_n\}$ una base di V: per il teorema \ref{app lineare e isomorfismo (basi in basi)}, $\exists$ un'unica appl. lineare che manda $f(v_1) \rightarrow v_1... f(v_n)\rightarrow v_n$ ed é proprio $f^{-1}$.\\
Inoltre $f^{-1}$ é biunivoca perché la sua inversa é f. $\#$\\\\
Abbiamo detto che due spazi vettoriali V,U sono isomorfi se esiste un isomorfismo $V\rightarrow U$ e scriviamo $\boldsymbol{V \simeq U}$.\\\\
\textbf{Prop}: "essere isomorfi" é una relazione di \underline{equivalenza}:
\begin{itemize}
    \item \underline{Riflessiva}: ogni spazio vettoriale V é isomorfo a sé stesso.\\
    Relazione identità: $V\longrightarrow V$, $v\longrightarrow v$
    \item \underline{Simmetrica}: se $V \simeq U$, allora $U \simeq V$.\\
    Se $f: V \longrightarrow U$ é un isomorfismo, abbiamo appena dimostrato che $f^{-1}:U\longrightarrow V$ é un isomorfismo.
    \item \underline{Transitiva}: se $V\simeq U$, $U\simeq W$ allora $V \simeq W$.\\
    Se $g: V\longrightarrow U$ e $f:U\longrightarrow W$ sono isomorfi allora anche $f\circ g: V\longrightarrow W$ é un isomorfismo.
    
\end{itemize}

\noindent\textbf{Corollario}: Due spazi vettoriali V,U su uno stesso campo $\mathbb{K}$ sono isomorfi $\iff$ hanno la stessa dimensione.\\
\textbf{Dim}: Sia $\{v_1..v_n\}$ una base di V e $\{u_1...u_m\}$ una base di U. Dunque dimV=n, dimU=m.\\
Se n=m $\exists$ una biezione $v_1\rightarrow u_1... v_n\rightarrow u_n$ che per il teorema \ref{appl lineare unica} si estende ad un'unica appl. lineare $f:V\longrightarrow U \;/\; f(v_1) = u_1...f(v_n) = u_n$ che per il teorema \ref{app lineare e isomorfismo (basi in basi)} é un isomorfismo (manda basi in basi).\\
Se $n\neq m$ non può esistere un isomorfismo $f:V\longrightarrow U$ perché altrimenti $\{f(v_1)...f(v_n)\}$ dovrebbe essere una base di U, ma questo é assurdo. \#\\\\
\underline{OSS}: Sia $f:V\longrightarrow U$ un'appl. lineare. Fissate basi $B_v, B_u$ di V e U, dato $v\in V$, il vettore delle coordinate di $f(v)$ nella base $B_u$ é ottenuto moltiplicando la matrice di f (nelle basi $B_v, B_u)$ per il vettore colonna delle coordinate di v nella base $B_v$.\\\\
Esempio:\\
Sia $V = \{p(x) \in \mathbb{R}[x] \;/\; p(x)\text{ha grado} \leq 3\} = \{\underbrace{ax^3+bx^2+cx+d}_{p'(x) = 3ax^2+2bx+c}\}$\\
$d:V\longrightarrow V\quad$ La derivata é un'applicazione lineare\\
$p(x)\rightarrow p'(x)$\\
Scriviamo la matrice D di d nella base canonica $\{1,x,x^2,x^3\}$\\

\noindent$d(1)=0$\\
$d(x)=1+0x+0x^2 + 0x^3 = 1$\\
$d(x^2)=0+2x+0x^2+0x^3= 2x$\\
$d(x^3) = 0+0x+3x^2+0x^3$\\
$D=
\begin{pmatrix}
    0 & 1 & 0 & 0\\
    0 & 0 & 2 & 0\\
    0 & 0 & 0 & 3\\
    0 & 0 & 0 & 0
\end{pmatrix}
$
$\cdot
\left(\begin{matrix}
    a\\
    b\\
    c\\
    d
\end{matrix}\right) = 
$
$
\left(\begin{matrix}
    c\\
    2b\\
    3a\\
    0
\end{matrix}\right)\\
$
Sono le coordinate di $p'(x)$ nella base $\{1,x,x^2,x^3\}$
\section{Sezione quarta}
\subsection{Matrice identità}
Sia $id: V\longrightarrow V$ l'applicazione lineare che manda ogni vettore in sè stesso, cioè:
$$id(v) = v \;\forall v \in V$$
Sia $v_1...v_n$ una base di V. Chi è  la matrice di id?\\
$f(v_1) = v_1= 1v_1+0v_2...$\\
$f(v_2) = v_2 = 0v_1 + 1v_2...$\\
$f(v_n) = v_n = 0v_1 + 0v_2 ... + 1v_n$\\\\
\textbf{In} = $\begin{pmatrix}
    1 & 0 & 0 &... & 0\\
    0 & 1 & 0 &... & 0\\
    0 & 0 & 1 &... & 0\\
    0 & 0 & 0 &... & 1
\end{pmatrix}
\text{In é la matrice identità, ha tutti 0 tranne gli 1 sulla diagonale.}
$
\\\\\\
\underline{OSS 1}: Ottengo In qualsiasi sia la base scelta.\\\\
\underline{OSS 2}: Per ogni $f:V \longrightarrow V$, $f\circ id = f$, $id \circ f = f$.\\
Esempio:\\
Ovvero, se A é la matrice di f, $A \cdot In = A, \quad In\cdot A = A$\\
In é l'elemento neutro del prodotto riga per colonna di matrici.\\\\
\underline{OSS 3}: $f:V\longrightarrow U$ lineare e sia n = dimV, m = dimU\\
f é isomorfo $\iff$ f é invertibile cioé $\forall u \in U \exists! v\in V \;/\; f(v) = u$\\
e scirvo $v = f^{-1}(u) \iff$ esiste $f^{-1}:U\longrightarrow V \;/\; f\circ f^{-1} = id_u, f^{-1}\circ f = id_v$.\\
Fissate basi di V e di U, se A é la matrice di f e $A^{-1}$ é la matrice di $f^{-1}$:
$$A\cdot A^{-1} = Im$$

\noindent \textbf{\underline{Def}}: $A^{-1}$ é detta la \textbf{matrice inversa} di A:
$$A^{-1}\cdot A = In$$

\noindent \underline{OSS 4}: f é un isomorfismo $\iff$ la sua matrice A é invertibile, cioé esiste $A^{-1}$ tale che valgono le equzioni scritte sopra.\\\\
\underline{OSS 5}: Se A non é quadrata (cioé righe $\neq$ colonne) non ha senso chiedersi se A sia invertibile perchè A corrisponde ad una funzione $f:V \longrightarrow U$ con $dimV \neq dimU$ e tale f non può essere biunivoca per il corollario del teorema del rango.\\

\noindent Sia $f:V \longrightarrow V$ un'applicazione lineare e siano $B = \{v_1...v_n\}$ e $B' = \{u_1...u_n\}$ basi di V. Sia A la matrice di f nella base B e sia M la matrice di f nella base B'.\\
Che legame c'è tra A e M?\\\\
\underline{\textbf{Def}}: La matrice di \textbf{cambiamento di base} é:

$B = \begin{pmatrix}
    ... & a_{ij} & ...\\
    ... & ...& ...\\
    ... & ... & ...\\
    ... & a_{nj} & ...
\end{pmatrix}$
\text{La cui j-ma colonna é ottenuta} $u_j = a_{1j}v_1+...+a_{nj}v_n$\\
\\
\begin{theorem}
    $$\boldsymbol{M = B^{-1}AB}$$

\label{matrici simili}
\end{theorem}
\subsection{Matrici invertibili}
\textbf{\underline{Def}}: Due matrici A,M sono \textbf{simili} se esiste una matrice invertibile B tale che $M = B^{-1}AB$.\\\\
\underline{OSS}: Per il teorema \ref{matrici simili}, due matrici sono simili $\iff$ rappresentano la stessa applicazione lineare (in basi diverse).\\\\
\underline{OSS}: essere simili é una relazione di equivalenza:\\
\textbf{Dim}:
\begin{itemize}
    \item Riflessiva $\Rightarrow$ $A = In^{-1}AIn$
    \item Simmetrica $\Rightarrow$ se $M = B^{-1}AB$ allora $A = BMB^{-1}$ (perché $B^{{-1}^{-1}} = B$)
    \item Transitiva $\Rightarrow$ se $M = B^{-1}AB$ e $N = C^{-1}MC$ allora $N=C^{-1}(B^{-1}AB)$, $C = (BC)^{-1}A(BC)$
\end{itemize}

Come capire se una matrice quadrata A é invertibile?
\begin{theorem}
    Una matrice quadrata A é \textbf{invertibile} $\iff$ i suoi vettori colonna sono linearmenti indipendenti.
    \label{matrici invertibili}
\end{theorem}

\noindent \textbf{Dim}: Siano $v_1...v_n$ i vettori colonna di A. \\
Consideriamo l'applicazione lineare $f: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ tale che $f(e_1) = v_1...f(e_n) = v_n$ essa esiste ed é unica per il teorema \ref{appl lineare unica}.\\La matrice di f nella base canonica é proprio $A = (v_1\;...\;v_n)$, perché\\ 
$v_1 = (x_1...x_n) = x_1e_1+...+x_ne_n$ allora $f(e_1) = v_1 = x_1e_1+...+x_ne_n$,\\
quindi per il teorema \ref{app lineare e isomorfismo (basi in basi)} $v_1...v_n$ é una base $\iff$ f é un isomorfismo $\iff$ la matrice A di f é invertibile. \#
\subsubsection{Il rango e la trasposta}
Sia A una matrice mxn.\\
\underline{\textbf{Def}}: Il \textbf{rango} di A (che indichiamo con rg(A)) é massimo numero di colonne linearmente indipendenti di A.\\\\
Esempio:\\\\
$A = \begin{pmatrix}
    2 & 0 & 3 & 0 & 3 & 1\\
    0 & -1 & 0 & 2 & 2 & 1\\
     0 & 0 & 0 & 0 & 0 & 0
\end{pmatrix}$
rg(A) = 2\\\\

\noindent Abbiamo dimostrato che una matrice nxn é invertibile $\iff$ ha rango n.\\
Sia $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ (o più in generale potremo prendere 
$f:\underbrace{V}_{dim_n}  \longrightarrow \underbrace{U}_{dim_m}$) e sia A la matrice associata ad f (nelle basi canoniche).
\begin{theorem}
    $$\text{dim Imf = rg(A)}$$
    \label{rango e dimensione immagine di f}
\end{theorem}
\textbf{Dim}: per definizione $f(e_1) = v_1...f(e_n)=v_n$ dove $v_1...v_n$ sono vettori colonna di A.\\
Poiché $e_1...e_n$ generano $\mathbb{R}^n$, $v_1...v_n$ generano Imf (perché $\forall v \in \mathbb{R}^n, \exists! a_1...a_n \;/\; v = a_1e_1+...+a_ne_n$ e quindi $f(v) = a_1v_1+...a_nv_n \forall f(v) \in Imf$.\\
Quindi il massimo numero di vettori linearmente indipendenti in $\{v_1...v_n\}$ é pari alla dimensione dell' immagine.\\
Nell'esempio sopra elencato Imf = $\{(x,y,z) \in \mathbb{R}^3 \;/\; z = 0\}$ che ha per base i primi due vettori colonna di A. \#\\\\
\underline{\textbf{Def}}: la \textbf{trasposta} di una matrice A é la matrice $A^T$ che ha per righe le colonne di A. In altre parole $(A^T)_{ij} = A_{ji}$\\\\
Esempio:\\
$A = \begin{pmatrix}
    1 & 0 & -2 & 5\\
    2 & 0 & -4 & 10\\
    0 & 1 & 1 & 1 
\end{pmatrix}
\quad A^T\begin{pmatrix}
    1 & 2 & 0\\
    0 & 0 & 1\\
    -2 & -4 & 1\\
    5 & 10 & 1
\end{pmatrix}
$
\\\\
\subsection{Determinante}
Sia A una matrice nxn. Il \textbf{determinante} di A é definito nel seguente modo ricorsivo.
\begin{itemize}
    \item se n = 2, $A = \begin{pmatrix}
        a & c\\
        b & d
    \end{pmatrix}
    $
    det(A) = ad-bc
    \item se n $>$ 2, riconduciamo il calcolo di det(A) al calcolo di determinanti di matrici più piccole mediante la \underline{\textbf{regola di Laplace}}:\\
    scegliamo una colonna di A, la j-ma,\\
    det(A) = $\displaystyle\sum_{i = 1}^{n} (-1)^{i+j} a_{ij} det(A^{ij})$ dove $A^{ij}$ é la matrice ottenuta da A rimuovendo la i-ma riga e la j-ma colonna.\\
\end{itemize}
\subsubsection{Proprietà del determinante}
\begin{enumerate}
    \item é \textbf{multilineare} cioé se ho due matrici che differiscono solo per una colonna allora,
    $A = (v_1...v_j...v_n), B = (v_1...v'_j...v_n) \Rightarrow$  $det(v_1av_j+bv'_jv_n) = a\dot det(A)+b \cdot det(B)$
    \item é alternante cioé se $A'$ é ottenuta da A scambiando due colonne, allora det($A'$) = -det(A)
    \item det(In) = 1
    \item é l'unica funzione $M_{n,n} \longrightarrow \mathbb{K}$ che verifica 1,2,3
    \item $\boldsymbol{det(A^T) = det(A)}$\\
    in particolare lo sviluppo di Laplace si può fare per righe e le proprietà 1,2 valgono anche per le righe.
    \item $det(A\cdot B) = det(A)det(B)$
    \item $\boldsymbol{det(A) \neq 0 \iff A}$ \textbf{é invertibile}
\end{enumerate}
\underline{OSS}: Per le proprietà 3 e 6, se A è invertibile $\boldsymbol{det(A^{-1}) = (det(A))^{-1}}$.\\
Infatti $AA^{-1} = In$ e quindi $det(AA^{-1}) \overbrace{=}^{6} det(A)det(A^{-1}) = det(In) \overbrace{=}^{3}1 $\\\\
\textbf{Prop}: Se A e M sono \textbf{simili}, allora hanno lo stesso determinante.\\
\textbf{Dim}: per definizione, se A e M sono simili esiste una matrice invertibile B tale che $M = B^{-1}AB$.\\
Quindi det(M) = det($B^{-1})det(A)det(B) = \underbrace{det(B)^{-1}det(B)}_{1}det(A) = det(A)$.\#\\\\
\underline{\textbf{Def}}: Sia V uno spazio vettoriale e $f:V\longrightarrow V$ una applicazione lineare.\\
Definiamo \textbf{det(f) = det(A)}, dove A é la matrice di f in una base di V. Per la prop precedente non dipende dalla base scelta.
\\\\
Il determinante ci permette di stabilire se una matrice quadrata A é invertibile o no. Ma come la troviamo in caso affermativo?
\begin{theorem}
    $$\boldsymbol{A^{-1} = \frac{1}{det(A)}\cdot cof(A)^T}$$
    dove $cof(A)_{ij} = (-1)^{i+j}det(A^{ij})$
\end{theorem}
\subsubsection{Prodotto vettoriale}
Il determinante ci dà anche una regola per calcolare il "prodotto vettoriale", un'operazione
$\mathbb{R}^3\times \mathbb{R}^3 \longrightarrow \mathbb{R}^3$, $(v,u) \longrightarrow v \wedge u $, usata in fisica.\\
Se $v= (a_1, a_2, a_3) , u=(b_1, b_2, b_3)$, definiamo:\\
$v\wedge u = det\begin{pmatrix}
    a_1 & a_2 & a_3\\
    b_1 & b_2 & b_3\\
    e_1 & e_2 & e_3
\end{pmatrix}$
$=(a_2b_3-a_3b_2)e_1 + (-a_1b_3+a_3b_1)e_2 + (a_1b_2-a_2b_1)e_3$

\section{Sezione quinta}
\subsection{Sistemi lineari matriciali: metodi risolutivi}
Consideriamo un sistema di \textbf{m} equazioni lineari in \textbf{n} incognite $x_1...x_n$:\\
$\begin{cases}
a_{11}x_1+a_{12}x_2+...+a_{1n}x_n = b_1\\
a_{21}x_1+a_{22}x_2+...+a_{2n}x_{n} = b_2\\
............\\
a_{m1}x_1+a_{m2}x_2+...+a_{mn}x_{n} = b_{m}
\end{cases}
$
\\\\
Possiamo riscrivere questo sistema "in forma matriciale" come:\\
$\boldsymbol{Ax = b}$ dove $x = \begin{pmatrix}
    x_1\\
    x_2\\
    ...\\
    x_n
\end{pmatrix}$
è il \underline{vettore delle incognite}, $b = \begin{pmatrix}
    b_1\\
    b_2\\
    ...\\
    b_m
\end{pmatrix}$
è  il \underline{vettore dei termini noti} e A è la matrice (m x n) dei coefficienti.\\
Come posso risolvere il sistema?
\begin{itemize}
    \item \textbf{\underline{1° metodo}(di Cramer)} $\Rightarrow$ \underline{applicabile sono se m = n} (cioè matrice quadrata). Ho due casi:
    \begin{enumerate}
        \item se $\boldsymbol{det(A) \neq 0}$, allora A è invertibile cioè $\exists A^{-1} \;/\; A^{-1}A = In$.\\
        Quindi $A^{-1}Ax = A^{-1}b \Rightarrow \boldsymbol{x = A^{-1}b}$ (ho moltiplicato per $A^{-1}$ entrambe le parti e poi semplificato $A^{-1}A = In$).\\
        Cioé posso ottenere il vettore delle soluzioni x = $\begin{pmatrix}
            x_1\\
            ...\\
            x_n
        \end{pmatrix}$
        moltiplicando il vettore dei termini noti b per l'inversa di A.

        \item Se $\boldsymbol{det(A) = 0}$ allora A non è invertibile cioè l'applicazione\\ $f:\mathbb{R}^n \longrightarrow \mathbb{R}^m$, \textbf{f(x)= Ax non é suriettiva o non è iniettiva}. A seconda del termine noto la soluzione del sistema non esiste(non suriettiva) o non è unica(non iniettiva).
        
    \end{enumerate}
    \item \textbf{\underline{2° metodo}(di Gauss)} \underline{Computazionalmente più efficiente e m può essere diverso da n}.\\
    Lavoriamo sulla matrice $\boldsymbol{A|b}$ ottenuta affiancando la colonna b ad A.\\
    \begin{enumerate}
        \item Il mio obiettivo è quello di ottenere una matrice che nella prima colonna ha tutti 0 tranne il primo numero. \\Possiamo supporre $a_{11}\neq 0$ (altrimenti scambiamo la prima equazione con un'altra). Moltiplico la prima riga per $a_{11}^{-1}$ ottenendo:\\
        $1\frac{a_{12}}{a_{11}}...\frac{a_{1n}}{a_{11}}=\frac{b_1}{a_{11}}$\\
        Sostituisco la seconda riga con (2° riga - $a_{21}\cdot$ la 1° riga) e così via con le altre... sostituisco la m-esima riga con (m-esima riga -$a_{m1}\cdot$ la 1° riga).\\
        Ottengo così una matrice $\begin{pmatrix}
            1 & * & ... & *\\
            0& * & ... &  *\\
            0 & * & ... & *\\
            ... & ... & ... & ...
            
        \end{pmatrix} \text{nella prima colonna tutti 0 tranne il primo 1}$
        \item Faccio lo stesso con la matrice $A^{11}$ in modo da ottenere anche qui tutti 0 nella prima colonna tranne il primo 1 (di $A^{11}$).\\
    \end{enumerate}
    Ad ogni passo ottengo una colonna $\begin{pmatrix}
        1\\
        0\\
        ...\\
        0
    \end{pmatrix}$
    oppure, $\begin{pmatrix}
        0\\
        0\\
        ...\\
        0
    \end{pmatrix}$
    se i coefficienti di $x_i$ sono tutti nulli.\\
    La matrice così ottenuta è molto semplice e corrisponde ad un sistema che  ha le stesse soluzioni di $Ax = b$, perché \textbf{le operazioni effettuate} (scambiare tra loro due righe, sommare ad una riga un multiplo dell'altra) \textbf{non cambiano le soluzioni}.\\
    
\end{itemize}
\begin{figure}[h!]
        \centering
        \includegraphics[width = 17cm]{RISULTATO 1.jpg}
        \caption{Sistema risolto con metodo Gauss}
        \label{sistema}
    \end{figure}
Nell'esempio in figura \ref{sistema} il sistema Ax = b é equivalente al sistema:\\
$\begin{cases}
    x_1 = -\frac{1}{2}x_3 - x_4+\frac{1}{2}x_5-\frac{1}{2}\\
    x_3 = 2x_4
- x_5 -\frac{1}{2}    
    \end{cases}$
    \\\\
cioé posso sciegliere come voglio i valori di $x_2,x_4,x_5$ ma $x_1,x_3$ sono deterinati da essi.\\
se $x_2 = t, x_4 = s, x_5 = r\; (t,s,r \in \mathbb{R})$ allora\\
$x_3 = 2s-r-\frac{1}{2}$, $x_1 = -\frac{1}{2}(2s-r-\frac{1}{2})-s+\frac{1}{2}r - \frac{1}{2}$.\\
L'insieme delle soluzioni è: $S = \{(-\frac{1}{2}(2s-r-\frac{1}{2})-s+\frac{1}{2}r - \frac{1}{2}, t, 2s-r-\frac{1}{2},s,r)\}$.\\\\
Nell'esempio precedente abbiamo trasformato la matrice A in una matrice $A'$. Osserviamo che $rg(A')$=max numero di colonne lin. indipendenti = 2 = numero di \textbf{Pivot} = numero di righr di $A'$ lin. indip.\\
D' altra parte $rg(A') = rg(A)$ perché le operazioni dell' algoritmo di Gauss non cambiano il rango. Lo stesso vale in generale.
\begin{theorem}
    Il \textbf{rango di A} (cioé il max n° di colonne lin. indip.) è uguale al max n° di righe lin indip, e anche al \textbf{numero di pivot} della matrice $A'$ ottenuta applicando ad A l'algoritmo di Gauss.
\end{theorem}
 
\subsubsection{Inverso di una matrice (metodo Gauss)}
\textbf{L'algoritmo di Gauss} ci permette anche di calcolare \textbf{l'inverso di una matrice quadrata}, se esiste applichiamo l'algoritmo a $A|In$ fino a ottenere $In|B$. $B$ sarà proprio $A^{-1}$.

\begin{theorem}[Rouchè-Capelli]
    Il sistema Ax = b ammette almeno una soluzione $\iff$ $rg(A|b) = rg(A)$.
\end{theorem}
\textbf{Dim}:\\
Ax = b ammette soluzioni $\iff$ b é comb lineare dei vettori colonna di A (con coefficienti $x_1...x_n$) $\iff$ il max n° di colonne lin. indip di $A|b$ é lo stesso di A $\iff$ rg($A|b) = rg(A)$.
\subsection{Insieme delle soluzioni di un sistema lineare}
Diciamo che un sistema è \textbf{\underline{omogeneo}} se $b_1=0, b_2=0...b_n=0$, cioè se è del tipo $Ax=\underline{0}$.\\\\
\textbf{Prop:} \textbf{L'insieme U delle soluzioni} del sistema lin. omogeneo $Ax = \underline{0}$ è un sottospazio vettoriale di $\mathbb{R}^n$, e dim U = n-rk(A).\\
\textbf{Dim:} $A\underline{0}=\underline{0}$ quindi \underline{0} $\in U$. Inoltre se $x,x' \in U$ (cioè $Ax=\underline{0}, Ax'=\underline{0}$) allora $A(x+x')=Ax+Ax'=\underline{0}+\underline{0}=\underline{0}$, quindi $x+x' \in U$.\\
Infine, se $x \in U$ e $a \in \mathbb{R}$, $A(ax) = a(Ax)= a\underline{0} = \underline{0} \Rightarrow ax \in U$, U è un SSV.\\
Abbiamo dimostrato che U è un sottospazio vettoriale. D'altra parte U è il nucleo (elementi del dominio che vanno in 0) dell'applicazione lineare\\ $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$, ($x \longrightarrow Ax$) dunque dimU = dimKerf $\overbrace{=}^{T.Rango}$ n-dimImf= n-rg(A). \#\\\\
\textbf{E se il sistema non è omogeneo?}\\
Esempio:\\
$S=\{(x_1,x_2)\in \mathbb{R}^2 \;/\; x_1+x_2 = 2\}$ il nostro sistema lineare.\\
$U = \{(x_1,x_2)\in \mathbb{R}^2 \;/\; x_1+x_2 = 0\}$ \underline{sistema omogeneo associato}.\\\\
\underline{OSS 1)}: U é un SSV di $\mathbb{R}^2$, S non lo è, ad esempio $(1,1) \in S, (0,2) \in S$, ma $(1,1) + (0,2) = (1,3) \notin S$.\\\\
\underline{OSS 2)}: Però $(1,1) - (0,2) = (1,-1) \in U$. Più in generale, se $x,x' \in S allora x-x' \in U$.\\\\
\underline{OSS 3)}: In altre parole, se $x \in S$, S si ottiene sommando x a tutti gli elementi di U. Diciamo che \underline{S è ottenuto traslando U per il vettore x}.
\begin{figure}[h!]
        \centering
        \includegraphics[scale = 0.4]{esempio2.png}
        \caption{Esempio traslazione insieme per un vettore}
        \label{esempio traslazione}
\end{figure}
\\\\\\
\subsection{Sottospazio affine}
Diciamo che S è un \textbf{sottospazio affine} di $\mathbb{R}^n$ se si ottiene traslando un sottospazio vettoriale cioè se $\exists U$ ssv di $\mathbb{R}^n, x\in S \;/\; \boldsymbol{S=\{u+x, u\in U\}}$.\\
Per definizione dimS = dimU.\\
\begin{theorem}
    L'insieme S delle soluzioni di un sistema lineare Ax = b, se è non vuoto, è un sottospazio affine di $\mathbb{R}^n$, ottenuto traslando per un qualsiasi $x \in S$ l'insieme U delle soluzioni del sist. lin. omogeneo Ax = \underline{0}. 
\end{theorem}
Ovvero $S=\{u+x, u\in U\}$ dove x è una qualsiasi soluzione.\\
\textbf{Dim}: Se $x,x' \in S$ allora $x-x' \in U$: infatti se $Ax = b$, $Ax' = b$ allora $A(x-x')=Ax-Ax'=b-b=\underline{0} \Rightarrow x-x' \in U$.\\
D'altra parte se $x\in S, u\in U$ allora $x+u \in S$, perchè $A(x+u) = Ax+Au=b+\underline{0} = b$.\#\\
\underline{OSS}: dim S = dim U = n - rgA.\\

\noindent ESEMPIO:\\
Sia S l'insieme delle soluzioni del sistema\\
$\begin{cases}
    x_1-x_2+x_3 = 2\\
    x_1+x_2+x_3 = 6\\
    2x_1+2x_3 = 8
    
\end{cases}
\text{sistema omogeneo associato} \Rightarrow
\begin{cases}
        x_1-x_2+x_3 = 0\\
        x_1+x_2+x_3 = 0\\
        2x_1+2x_3 = 8
    \end{cases}
    \iff\\
    \begin{cases}
        x_3 = -x_1\\
        x_2 = 0\\
        x_3 = -x_1
    \end{cases}\\\\
$
Ha per insieme le sol: $U = \{(x_1,x_2,x_3) \in \mathbb{R}^3 \;/\; x_1=-x_3, x_2=0\}=$\\
$=\{(t,0,-t), t\in \mathbb{R}\}$\\
Si vede facilmente che (1,2,3) appartiene a S:\\ 
$S=\{u+(1,2,3), u\in U\}=\{(t+1, 2, -t+3)\}$

\section{Sezione sesta}
\subsection{Matrice diagonale}
Sia V uno spazio vettoriale su campo $\mathbb{K}$ (e sia n=dimV).\\
\textbf{Def}: Un \textbf{endomorfismo} di V è un'applicazione lineare $V \longrightarrow V$.\\
Sia f un endomorfismo di V. Abbiamo visto che, fissata una base di V, possiamo associare ad f una matrice nxn, che dipende dalla base scelta.\\
\textbf{Domanda}: Posso scegliere la base di V in modo che la matrice di f sia \underline{particolarmente semplice}?\\\\
\underline{\textbf{Def}}: Una matrice \textbf{D(nxn)} è \textbf{\underline{diagonale}} se $a_{ij} = 0 \forall i \neq j$.\\\\
D=
$\begin{pmatrix}
  d_1 & 0 & ... & 0\\
  0 & d_2 & ... & 0\\
  ... & ... & ... & ...\\
  0 & 0 & ... & d_n
\end{pmatrix}
= \begin{pmatrix}
    d_1 & & \\
    & \ddots &  \\
    & & d_n 
\end{pmatrix}
d_1, d_n \in \mathbb{K}\\\\
$
\underline{OSS}:Due matrici diagonali D,E sono facili da moltiplicare tra loro! Infatti se D è come sopra ed\\
E=$\begin{pmatrix}
    e_1 & & \\
    & \ddots & \\
    & & e_n
\end{pmatrix}
\quad D\cdot E = \begin{pmatrix}
    d_1e_1 & & \\
    & \ddots & \\
    & & d_ne_n
\end{pmatrix}\\
$
\subsection{Autovettore e autovalore}
Sia $f: V \longrightarrow V$ un'applicazione lineare.\\
\textbf{Def}: Un \textbf{\underline{autovettore}} di f è un $v \in V, v \neq \underline{0}\;/\; \boldsymbol{f(v)=\lambda v}$ per qualche $\lambda \in \mathbb{K}$.\\
(In altre parole, $\lambda \in \mathbb{K}$ é una \textbf{\underline{autovalore}} se $\exists v \in V$, $v \neq \underline{0},\;/\; f(v)=\lambda v$ e tale v é detto \textbf{autovettore relativo} a $\lambda$).\\\\
Esempio:\\
$f:\mathbb{R}^2 \longrightarrow \mathbb{R}^2$ $f(x,y)=(y,x)$\\
$v_1 = (1,1)$ è un autovettore di autovalore 1 perché $f(v_1)=1v_1$\\
$v_2 = (1,-1)$ è un autovettore di autovalore -1 perché $f(v_2) = -1v_2$\\
La matrice di f nella base $v_1,v_2$ è $\begin{pmatrix}
    1 & 0\\
    0 & -1
\end{pmatrix}
f(v_1) = 1v_1+0v_2\quad f(v_2) = 0v_1-1v_2\\\\
$
In generale se $\{v_1,v_2...v_n\}$ è una base di V composta da autovettori, allora la matrice di f in tale base è diagonale, perché:\\
$f(v_1)=\lambda_1v_1+0v_2...+0v_n$\\
$f(v_2)=0v_1+\lambda_2v_2...+0v_n \quad \quad\begin{pmatrix}
    \lambda_1 & & \\
    & \ddots &\\
    & & \lambda_n
\end{pmatrix}$\\
$f(v_n)=0v_1...+\lambda_nv_n$\\\\
Quindi per "diagonalizzare f" (cioè rappresentare f con una matrice diagonale), dobbiamo trovare una base di autovettori. Come si fa?\\
Procediamo in 2 passi:
\begin{enumerate}
    \item Troviamo gli autovalori di f.
    \item Troviamo gli autovettori corrispondenti.
\end{enumerate}
\textbf{\underline{Def}}: \textbf{Polinomio caratteristico} di f:
$$\boldsymbol{p(\lambda)=det(f-\lambda id)}$$
\begin{theorem}
    $\lambda_i$ è un autovalore di f $\iff p(\lambda_i) = 0$. 
\end{theorem}
\textbf{Dim}: $\lambda_i$ è un autovalore di f $\iff \exists v \in V, v\neq \underline{0} \;/\; f(v) = \lambda_iv \iff \\ \exists v\in V, v\neq \underline{0} \;/\;  f(v) = \lambda_iv \Rightarrow f(v) - \lambda_iid(v) = 0 \; (\text{perchè}\; \lambda_iid(v) = \lambda_i(v))\Rightarrow (f-\lambda_iid)v = \underline{0}\iff  \exists v \neq \underline{0} \;/\;, v\in Ker(f-\lambda_iid) \iff f-\lambda_iid$ non è iniettiva, ovvero non è un isomorfismo $\iff p(\lambda_i)=det(f-\lambda_i id) = 0$.\#\\
Quindi per trovare gli autovalori di f basta trovare gli zeri del polinomio $p(\lambda)!$\\

\noindent Esempio:\\
$f:\mathbb{R}^2 \longrightarrow \mathbb{R}^2$, $f(x,y)=(y,x)$\\
Per calcolare $p(\lambda)$, scriviamo la matrice A di f in una base a nostra scelta (ad es la base canonica).\\
$f(e_1) = 0e_1 + 1 e_2 \quad\quad\quad A= \begin{pmatrix}
    0 & 1\\
    1 & 0
\end{pmatrix}$\\
$f(e_2) = 1e_1 + 0e_2$\\
La matrice di $f-\lambda id$ è $A-\lambda I_2 = \begin{pmatrix}
    0 & 1\\
    1 & 0
\end{pmatrix} -\lambda \begin{pmatrix}
    1 & 0\\
    0 & 1
\end{pmatrix} = \begin{pmatrix}
    -\lambda & 1\\
    1 & -\lambda
\end{pmatrix}$\\
$p(\lambda) = det\begin{pmatrix}
    -\lambda & 1\\
    1 & -\lambda
\end{pmatrix} = \lambda^2 - 1 = (\lambda+1)(\lambda-1)$\\
$p(\lambda) = 0 \iff \lambda = 1 \;\text{oppure}\;\lambda = -1$\\
Abbiamo così ottenuto gli autovalori di f. A questo punto trovare gli autovettori è facile, basta usare la definizione.\\
Sia $\lambda_1 = 1$, cerco $v_1 \;/\; f(v_1) = \lambda_1v_1 = v_1$\\
cioè cerco $(x,y) \;/\; (y,x) = (x,y)$:\\
$\begin{cases}
    y = x\\
    x = y
\end{cases}$
una soluzione è $x = 1, y = 1\Longrightarrow v_1 = (1,1).$\\\\
Analogamente dato $\lambda_2 = -1$ cerco $v_2 \;/\; f(v_2) = \lambda_2v_2 = -v_2$\\
cioè cerco $(x,y) \;/\; (y,x) = -(x,y)$:\\
$\begin{cases}
    y = -x\\
    x = -y
\end{cases}$ 
una soluzione può essere $x=1, y=-1 \Longrightarrow v_2 = (1,-1)$.\\\\
\underline{OSS}: tutto questo funziona se $p(A)$ che è un polinomio a coefficienti in $\mathbb{K}$ ha i suoi zeri in $\mathbb{K}$.
\begin{theorem}[Teorema fondamentale dell'algebra]
Ogni polinomio a coefficienti in $\mathbb{C}$ ha tutte le sue radici in $\mathbb{C}.$    
\end{theorem}
Quindi gli autovalori (che sono le radici del polinomio caratteristico $p(A)$, li posso sempre trovare in $\mathbb{C}$ ma non è detto che siano in $\mathbb{R}$.

\subsection{Autospazio}
 \textbf{\underline{Def}}: Sia $\lambda_i \in \mathbb{K}$ un autovalore, l'\textbf{autospazio} relativo a $\lambda_i$ è:$$ \boldsymbol{V_{\lambda_i} = \{v \in V \;/\; f(v) = \lambda_iv\}}$$
\\\\
 \textbf{Prop}:
 \begin{itemize}
     \item 1) $V_{\lambda_i}$ è un sottospazio vettoriale di V.
     \item 2) Se $\lambda_i \neq \lambda_j$, $V_{\lambda_i} \cap V_{\lambda_j} = \{\underline{0}\} $
 \end{itemize}
\textbf{Dim}:\\
\textbf{1)} Se $v_i, v_2 \in V_{\lambda_i}$ allora $f(v_1) = \lambda_iv_1$ e $f(v_2) = \lambda_iv_2$, $f(v_1+v_2) = f(v_1)+f(v_2) = \lambda_iv_1 +\lambda_iv_2 = \lambda_i(v_1+v_2) \Rightarrow v_1 + v_2 \in V_{\lambda_i}$.\\ Se $a\in \mathbb{K}, f(av_1) = af(v_1) = af(v_1) = a\lambda_iv_1 \Rightarrow av_1 \in V_{\lambda_i}$\\
\textbf{2)}Se $v \in V_{\lambda_i} \cap V_{\lambda_j}$, $\lambda_iv = f(v) = \lambda_jv \Rightarrow \lambda_iv - \lambda_jv = \underline{0}$, cioè $(\lambda_i-\lambda_j)v = \underline{0}$. Per ipotesi $\lambda_i \neq \lambda_j$ quindi $v = \underline{0}$. \#\\

\noindent \textbf{\underline{Def}}: Sia $\lambda_i \in \mathbb{K}$ un autovalore di f. 
\\La \textbf{molteplicità geometrica} di $\lambda_i$ è: $\boldsymbol{Mg(\lambda_i) = dim(V_{\lambda_i})}$.
\\La \textbf{molteplicità algebrica} di $\lambda_i$ è la molteplicità di $\lambda_i$ come soluzione del polinomio $p(\lambda) = 0$, ovvero il massimo intero m tale che $(\lambda-\lambda_i)^m$ divida $p(\lambda)$, e la indichiamo con $\boldsymbol{Ma(\lambda_i)}$.\\\\
Esempio:\\
$p(\lambda) = \lambda^4(\lambda-3)^2(\lambda-7) = 0$\\
$\lambda_1 = 0, \quad \lambda_2 = 3, \quad \lambda_3 = 7 \quad \quad Ma(0) = 4\quad Ma(3) = 2\quad Ma(7) = 1$\\

\noindent Esempio:\\
$f: \mathbb{R}^2 \longrightarrow \mathbb{R}^2 \quad f(x,y) = (2x+5y, 2y)$\\
$f(e_1) = 2e_1, \; f(e_2) = 5e_1 + 2e_2\quad A = \begin{pmatrix}
    2 & 5\\
    0 & 2
\end{pmatrix}$\\
$p(\lambda) = det\begin{pmatrix}
    2-\lambda & 5 \\
    0 & 2-\lambda
\end{pmatrix} = (2-\lambda)^2=0 \quad \quad \lambda_1 = 2\quad Ma(\lambda_1) = 2\quad Mg(\lambda_1)=?$\\
$V_{\lambda_1}=\{v\in V \;/\; f(v) = \lambda_1v\} = \{(x,y) \in \mathbb{R}^2 \;/\; (2x+5y, 2y) = (2x, 2y)\}$\\
$\begin{cases}
    2x+5y = 2y\\
    2y = 2y
\end{cases} \Rightarrow y=0$
quindi $V_{\lambda_1} = \{(x,0), x \in  \mathbb{R}\} \Rightarrow Mg(\lambda_1) = dim(V_{\lambda_1}) = 1$\\
In questo esempio \textbf{non} posso trovare una base di autovettori: $v_1 = (1,0)$ è un autovettore ma se prendo $v_2 \in V_{\lambda_1}$ allora $\{v_1,v_2\}$ non è una base perché i vettori non sono linearmente indipendenti. Se invece prendessi $v_1 \notin V_{\lambda_1}$, allora $v_2$ non sarebbe un autovettore.\\
f non si può diagonalizzare.\\\\
\textbf{Prop}: Sia $\lambda_i$ un autovalore di f. Allora:$$1 \boldsymbol{\leq Mg(\lambda_i) \leq Ma(\lambda_i)}$$
\textbf{Dim}: $Mg(\lambda_i) \leq 1$ perchè se fosse = 0 avremmo $V_{\lambda_i} = \{\underline{0}\}$ cioè $f(v) = \lambda_iv \iff v = \underline{0}$ cioè $\lambda_i$ non è autovalore. ASSURDO!\\
Ora sia m = $Mg(\lambda_i)$, per mostrare che $Ma(\lambda_i)\geq m$ prendiamo una base $\{v_1...v_m\}$ di $V_{\lambda_i}$ e completiamola ad una base di V.\\
$\{v_1...v_m, v_{m+1}...v_n\}$ e scriviamo la matrice di f in questa base:\\
$\begin{pmatrix}
 \lambda_i & & 0 & ? & ? & ?\\
    & \ddots & 0 & ? & ? & ?\\
    & & \lambda_i & ? & ? & ?\\
    0 & 0 & 0 & ? & ? & ?\\
    0 & 0 & 0 & ? & ? & ?\\
    0 & 0 & 0 & ? & ? & ?
\end{pmatrix}$\\\\
La prima parte della matrice in alto a sinistra è diagonale. Facendo lo sviluppo di Laplace rispetto alle prime m colonne ottengo $det(A-\lambda In) = (\lambda_i-\lambda)^m\cdot q(\lambda)$, dove $q(\lambda)$ è un polinomio che non conosco. Quindi in $p(\lambda)$ ci sono almeno m fattori $(\lambda_i -\lambda)$, potrebbe essercene qualcuno anche in $q(\lambda)$...forse.\#\\\\
\underline{OSS}: In particolare, se $\lambda_i \in \mathbb{K}$ ed ha $Mg(\lambda_i) = 1 \forall \lambda_i$
allora f di diagonalizza.
\begin{theorem}
    Sia $f: V\longrightarrow V$ un'applicazione lineare e siano $\lambda_1...\lambda_h$ i suoi autovalori distinti. Allora f è diagonalizzabile (cioè ammette base di autovettori) $\iff$ valgono le condizioni seguenti:
    \begin{enumerate}
        \item $\lambda_i \in \mathbb{K} \forall i=1...h$
        \item $Ma(\lambda_i) = Mg(\lambda_i) \forall i=1...h$
    \end{enumerate}
    
\end{theorem}
Cioè ogni autovalore deve appartenere al campo considerato ed ha molteplicità geometrica uguale a quella algebrica.\\\\
\textbf{Dim}:\\
Supponiamo che valga la 1) ovvero $\lambda_1...\lambda_h \in \mathbb{K}$ e consideriamo gli autospazi $V_{\lambda_1}...V_{\lambda_h}$. Per ciascuno di essi scegliamo una base $B_1...B_h$. Dato che se $\lambda_i \neq \lambda_j$, $V_{\lambda_i} \cap V_{\lambda_j} = \{\underline{0}\}$, gli autospazi formano somma diretta, sia U tale somma. $\underline{U} = V_{\lambda_1} \oplus... \oplus V_{\lambda_h}$. Dunque $B = B_1 \cup B_2 \cup... B_h$ è una base per $\underline{U}$. Dobbiamo capire se $U = V$ oppure $U \subsetneq V (\text{sottoinsieme proprio})$.\\
Se vale la 2) dim\underline{U} = $Mg(\lambda_1)+...+Mg(\lambda_h)$ = $Ma(\lambda_1)+...+Ma(\lambda_h)$ e dunque U=V cioè $B$ è una base di V composta da autovettori!.\\
Se non vale 2) allora $Mg(\lambda_1)+...+Mg(\lambda_h) < Ma(\lambda_1)+...+Ma(\lambda_h)$ e quindi dimU < dimV, perciò $B$ (che è una base di U) NON è una base di V. Posso completarla ad una base di V solo aggiungendo vettori che non sono autovettori. Quindi f non si diagonalizza.\\\\
\underline{OSS}: Se valgono 1) e 2) questo teorema ci dà un algoritmo per trovare una base di V composta da autovettori: basta trovare una base di ciascun autospazio, e poi unirle tutte.

\subsection{Applicazioni nilpotenti}
\textbf{\underline{Def}}: f è \textbf{nilpotente} se $\exists n>0 \;/\; f^n=f\circ...\circ f = \underline{0}$ (applicazione nulla).\\
A è nilpotente se $\exists n>0 \;/\; A^n = 0$ (matrice nulla).\\\\
Esempio:\\
$A=\begin{pmatrix}
    0 & 1 & 1\\
    0 & 0 & 1\\
    0 & 0 & 0
\end{pmatrix}\quad A^2 = \begin{pmatrix}
    0 & 0 & 1\\
    0 & 0 & 0\\
    0 & 0 & 0
\end{pmatrix}\quad A^3 = \begin{pmatrix}
    0 & 0 & 0\\
    0 & 0 & 0\\
    0 & 0 & 0
\end{pmatrix}$\\\\
\begin{theorem}
    Se $f\neq 0 $ è nilpotente, allora non è diagonalizzabile.
\end{theorem}
\textbf{Dim}: Se f è diagonalizzabile, esiste una base $\{v_1...v_n\} \;/\; f(v_i) = \lambda_iv_i \forall i=1...n$.\\
Se f è nilpotente, $\exists n>0 \;/\; f^n = 0$ e dunque $0 = f^n(v_i) = \lambda_i^nv_i \Rightarrow \lambda_i^n = 0 \Rightarrow \lambda_i = 0$ quindi f = 0.\#\\\\
\underline{OSS}: In altre parole, se f è nilpotente, il suo unico autovalore è 0!\\
\begin{theorem}[forma canonica di Jordan]
 Sia $f: V\longrightarrow V$ lineare. Allora esiste una base di V in cui la matrice di f è della forma:
 $\begin{pmatrix}
     J_{\lambda_1,n_1} & 0 & 0\\
     & \ddots & \\
     0 & 0 & J_{\lambda_h, n}
 \end{pmatrix}$
\end{theorem}

\section{Sezione settima}
\subsection{Forma bilineare}
\textbf{\underline{Def}}: Dato uno spazio vettoriale V su campo $\mathbb{K}$, una \textbf{forma bilineare} è una applicazione $\beta: V\times V \longrightarrow \mathbb{K}$ (cioè associa a due vettori $v,u \in V \text{un numero}\beta (v,u)\in \mathbb{K}$) che è lineare nel primo e nel secondo argomento, cioè:
$$\beta(v_1+v_2,u) = \beta(v_1,u)+\beta(v_2,u)$$
$$\beta(v,u_1+u_2) = \beta(v,u_1) + \beta(v, u_2)$$
$$\beta(av, u) ) = a\beta(v,u) = \beta(v,au)$$
Esempio:\\
$V = \mathbb{R}^2\quad v = (x_1,x_2), u= (y_1,y_2)$\\
$\beta(v,u) = 3x_1y_1-x_1y_2+2x_2y_1+x_2y_2$, è bilineare perché se prendo $v' = (z_1,z_2)$:\\
$\beta(v+v',u) = 3(x_1+z_1)y_1-(x_1+z_1)y_2+2(x_2+z_2)y_1+(x_2+z_2)y_2=3x_1y_1-x_!y_2+2x_2y_1+x_2y_2+3z1y_1-z_1y_2+2z_2y_1+z_2y_2 = \beta(v,u) + \beta(v',u)$\\
analogamente con le altre proprietà.\\\\
\textbf{\underline{Def}}: Una forma bilineare $\beta$ è \textbf{simmetrica} se $\beta(v,u) = \beta(u,v) \forall v,u \in V$.\\
$\beta$ è \textbf{antisimmetrica} se $\beta(v,u) ) - \beta(u,v) \forall v,u \in V$.\\\\
\underline{OSS}: Ogni forma bilineare $\beta$ si può scrivere come $\beta = \beta_s + \beta_a$, con $\beta_s$ simmetrica e $\beta_a$ antisimmetrica. Basta prendere:\\
$\beta_s(v,u) = \frac{\beta(v,u)+\beta(u,v)}{2}$\\
$\beta_a(v,u) = \frac{\beta(v,u)-\beta(u,v)}{2} $\\\\
Sia V uno spazio vettoriale $v_1...v_n$ una base di V e sia $\beta$ una forma bilineare.
Dati $v,u \in V$, posso scriverli nella base data:\\
$v = a_1v_1+...+a_nv_n =\displaystyle\sum_{i=1}^{n}a_iv_i  \quad u= b_1v_1+...+b_nv_n = \displaystyle\sum_{j=1}^{n}b_jv_j$\\
$\beta(v,u) = \beta(\displaystyle\sum_{i=1}^{n}a_iv_i,\displaystyle\sum_{j=1}^{n}b_jv_j) \overbrace{=}^{\text{lin 1°}} \displaystyle\sum_{i=1}^{n}{a_i\beta(v_i, \displaystyle\sum_{j=1}^{n}b_jv_j}) \overbrace{=}^{\text{lin 2°}} \displaystyle\sum_{i,j = 1}^{n}a_ib_j\beta(v_i,v_j)$\\
Mi basta sapere $\beta(v_i,v_j)$! Quindi posso rappresentare $\beta$ con una matrice A dove $A_{ij} = \beta(v_i, v_j)$.\\\\
Esempio:\\
$\beta(v,u) = 2x_1y_1+x_1y_2-x_2y_1+3x_2y_2$\\
$\beta(e_1,e_1) = 2 \quad \beta(e_1,e_2) = 1 \quad \beta(e_2,e_1) = -1 \quad \beta(e_2,e_2) = 3\quad A = \begin{pmatrix}
    2 & 1 \\
    -1 & 3
\end{pmatrix}$\\\\
Quindi se $v = a_1v_1+...+a_nv_n, \quad u = b_1v_1+...+b_nv_n$\\
$\beta(v,u) = (a_1,...a_n)A\begin{pmatrix}
    b_1\\
    ...\\
    b_n
\end{pmatrix}$

\subsubsection{Matrici simmetriche e matrici congruenti}
\textbf{\underline{Def}}: A è \textbf{simmetrica} se $A = A^T$ cioè se $A_{ij} = A_{ji}$, A è \textbf{antisimmetrica} se $A = -A^T$ cioè se $A_{ij} = -A_{ji} \forall i,j$.\\\\
Esempi:\\
$\begin{pmatrix}
    2 & 3 & 5\\
    3 & -1 & -3\\
    5 & -3 & 7
\end{pmatrix}$ è simmetrica $\begin{pmatrix}
    0 & 3 & 4\\
    -3 & 0 & -2\\
    -4& 2 & 0
\end{pmatrix}$\\\\\\
\underline{OSS} una forma bilineare è simmetrica $\iff$ la sua matrice (in qualnque base) è simmetrica.\\
Una forma bilineare è antisimmetrica $\iff$ la sua matrice è antisimmetrica.\\\\
\textbf{\underline{Def}}: Due matrici A,M sono congruenti $\iff$ $\exists$ una matrice invertibile $B \;/\; M = B^TAB$.\\
"Essere congruenti" è una relazione di equivalenza.
\begin{theorem}
    Due matrici sono congruenti se e solo se rappresentano la stessa forma bilineare (in basi magari diverse)
\end{theorem}
\textbf{Dim}: $v = a_1v_1+...+a_nv_n = x_1u_1+...+x_nu_n$\\
$u = b_1v_1+...+b_nv_n = y_1u_1+...+y_nu_n$\\\\
$\beta(v,u) = (a_1...a_n)A\begin{pmatrix}
    b_1\\
    ...\\
    b_n
\end{pmatrix} = \begin{pmatrix}
    a_1\\
    ...\\
    a_n
\end{pmatrix}^TA\begin{pmatrix}
    b_1\\
    ...\\
    b_n
\end{pmatrix}\quad$
ma $\begin{pmatrix}
    a_1\\
    ...\\
    a_n
\end{pmatrix} = B\begin{pmatrix}
    x_1\\
    ...\\
    x_n
\end{pmatrix}\quad\quad \begin{pmatrix}
    b_1\\
    ...\\
    b_n
\end{pmatrix} = B\begin{pmatrix}
    y_1\\
    ...\\
    y_n
\end{pmatrix}$\\\\
$\beta(v,u)=(B\begin{pmatrix}
    x_1\\
    ...\\
    x_n
\end{pmatrix})^TAB\begin{pmatrix}
    y_1\\
    ...\\
    y_n
\end{pmatrix}=\begin{pmatrix}
    x_1\\
    ...\\
    x_n
\end{pmatrix}^T\underbrace{B^TAB}_{M}\begin{pmatrix}
    y_1\\
    ...\\
    y_n
\end{pmatrix}  \quad\#$

\subsection{Diagonalizzare forme bilineari}
Analogamente a quanto fatto per le appl. lineari, ci chiediamo:\\
"posso trovare una base $v_1...v_n$ in cui la matrice della forma bilineare $\beta$ sia diagonale?\\
\underline{OSS}: Se $\beta$ è antisimmetrica, sicuramente no:\\
l'unica matrice diagonale antisimmetrica è la matrice nulla. (perchè una matrice antisimmetrica ha tutti 0 sulla diagonale).\\
\begin{theorem}
    Se $\beta$ è simmetrica, esiste una base $v_1...v_n $ in cui la matrice di $\beta$ è diagonale. In altre parole, ogni matrice simmetrica è congruente ad una matrice diagonale.
    \label{diagonalizzare forma bilineare simmetrica}
\end{theorem}
\textbf{Dim}: Sia $w_1...w_n$ una base qualsiasi:
\begin{itemize}
    \item PASSO 1) se $\beta(w_1,w_1) \neq 0$ vai al passo 2. Se $\beta(w_1,w_1) = 0$ ma esiste $i \;/\; \beta(w_i,w_i) \neq 0$, scambio $w_1$ con $w_i$ e vado al passo 2. Se $\beta(w_i, w_i)=0 \forall i$ allora cerco $i,j \;/\; \beta(w_i, w_j) \neq 0$ e scambio $w_1 = w_i+w_j\; w_2 = w_j\; w_i = w_1\; w_j = w_2 $ e vado al passo 2.
    \item PASSO 2) Faccio una nuova base.\\
    $w_1' = w_1, \forall i=2...n w_i' = w_i-\frac{\beta(w_1,w_i)}{\beta(w_1,w_1)}w_1$\\
    così che $\beta(w_1, w_i) = \beta(w_1,w_i) - \frac{\beta(w_1,w_i)}{\beta(w_1,w_1)}\beta(w_1,w_1)=0$.\\
    Ottengo una matrice che ha sulla prima riga e sulla prima colonna tutti 0 tranne $A_{1,1}$, per diagonalizzarla completamente devo ripetere il procedimento anche su $(w_2',...,w_n')$ senza toccare $w_1'$. Dopo n-1 iterazioni ottengo una matrice diagonale.
\end{itemize}
Con questo algoritmo è possibile diagonalizzare una forma bilineare simmetrica. Si può fare di meglio?\\
Esempio:\\
Sia $V = \mathbb{R}^4$, supponiamo di avere trovato una base $v_1,v_2,v_3,v_4$ tale che la matrice di $\beta$ è diagonale.\\
Se pongo $u_1 = \frac{1}{2}v_1, \beta(u_1,u_1) ) = \beta(\frac{1}{2}v_1, \frac{1}{2}v_1 = 1/4 \cdot 4 = 1$\\
Se pongo $u_2 = \frac{1}{\sqrt{3}}v_2, \beta(u_2,u_2) = 1$\\
Se pongo $u_3 = \frac{1}{\sqrt{5}}v_3, \beta(u_3,u_3) = -1$\\
Se pongo $u_4 = v_4, \beta(u_4,u_4) = 0$\\\\
$A = \begin{pmatrix}
    4 & 0 & 0 & 0\\
    0 & 3 & 0 & 0\\
    0 & 0 & -5 & 0\\
    0 & 0 & 0 & 0
\end{pmatrix}$ dopo le sostituzioni: $A = \begin{pmatrix}
    1 & 0 & 0 & 0\\
    0 & 1 & 0 & 0\\
    0 & 0 & -1 & 0\\
    0 & 0 & 0 & 0
\end{pmatrix}$\\
\underline{OSS}: se invece che su $\mathbb{R}^4$ fossi stato su $\mathbb{C}^4$ invece che $u_3 = \frac{1}{\sqrt{5}}v_3$ avrei potuto porre:\\ $u_3 = \frac{1}{\sqrt{-5}}v_3 = \frac{1}{\sqrt{5}}iv_3 $
e così $\beta(u_3,u_3) = 1$ e avrei avuto una matrice con solo 0 e 1.\\
\begin{theorem}[Sylvester]
Dato uno spazio vettoriale $V$ su $\mathbb{R}$ e una forma bilineare simmetrica $\beta$ esiste una base $u_1...u_n$ di V tale che la matrice di $\beta$ in tale base è:\\
$\begin{pmatrix}
    1 & & & & & & &\\
    & .. & & & & & & \\
    & & 1 & & & &  \\
    & & & -1 & & & \\
    & & & & .. & & & \\
    & & & & & -1 & & \\
    & & & & & & 0 & & \\
    & & & & & & & .. & \\
    & & & & & & & & 0
\end{pmatrix}$
Con p = num. di 1, r-p = num. di -1, n-r = num. di 0\\
Inoltre la coppia di numeri \textbf{(p, r-p)} non dipende dalla base scelta ed è detta la \textbf{segnatura} di $\beta$.
\end{theorem}
\textbf{Dim}: Per il teorema \ref{diagonalizzare forma bilineare simmetrica}, esiste una base $v_1...v_n$ in cui la matrice di $\beta$ è diagonale. A meno di riordinarli sappiamo che:\\
$\beta(v_i,vi)$ sia $\begin{cases}
    > 0 \forall i \in \{1...p\}\\
    < 0 \forall i \in \{p+1...r\}\\
    = 0 \forall i \in \{r+1...n\}
\end{cases}$\\\\
Allora ponendo $u_1 = \begin{cases}
    \frac{1}{\sqrt{|\beta(v_i,v_i)|}}v_i \text{ se} i \leq r\\
    v_i \text{altrimenti}
\end{cases} $\\\\
$\beta(u_i,u_i) = \begin{cases}
    1 i\in \{1...p\}\\
    -1 i \in \{p+1...r\}\\
    0 i \in \{r+1...n\}
\end{cases}\quad \#$\\
\begin{theorem}
    Dato uno spazio vettoriale V su $\mathbb{C}$ e una forma bilineare simmetrica, esiste una base $u_1...u_n$ di V tale che la matrice di $\beta$ è della forma $\begin{pmatrix}
        1 & & & & &  \\
        & .. & & & & \\
        & & 1 & & &\\
        & & & 0 & &\\
        & & & & .. &\\
        & & & & & 0
    \end{pmatrix}$\\
    Il numero di 1 è uguale al rango della matrice
\end{theorem}

\noindent\textbf{Dim}: Come nel teorema precedente, ma $\forall j\in \{p+1...r\}$ $u_j = \frac{1}{\sqrt{\beta(v_j,v_j)}}v_j = i\frac{1}{\sqrt{|\beta(v_j,v_j)|}}v_j$
in modo che $\beta(u_j,u_j) = 1. \quad \#$\\\\
I due teoremi precedenti, in altre parole, ci dicono che:\\
\textbf{Corollario}: Due matrici simmetriche sono congruenti su $\mathbb{R} \iff$ hanno la stessa segnatura; sono congruenti su $\mathbb{C} \iff$ hanno lo stesso rango.

\section{Prodotto scalare}
\subsection{Forma quadratica}
\textbf{Def}: Sia $\beta$ una forma bilineare simmetrica su $\mathbb{R}$. La \textbf{forma quadratica} associata a $\beta$ è $q:V\longrightarrow \mathbb{K}\quad q(v) = \beta(v,v)$.\\\\
\underline{OSS 1)}: Data q posso ritrovare $\beta$ perchè:\\
$(q(v+u)-q(v)-q(u))/2 = (\beta(v+u,v+u))-\beta(v,v)-\beta(u,u))/2$\\
$=(\beta(v,v)+\beta(u,u) +2\beta(v,u)-\beta(v,v) - \beta(u,u))/2 = \beta(v,u)$\\\\
\underline{OSS 2)}: data una forma quadratica $q : V \longrightarrow \mathbb{K},\; q(\underline{0}) = \beta(\underline{0}, \underline{0}) = \underline{0}$\\

\noindent Esempio:\\
$V = \mathbb{R}^2\quad v = (x_1,x_2), u=(y_1,y_2)\quad \beta(v,u) = 2x_1y_1-x_2y_2+3x_1y_2+3x_2y_1$\\
$q(v) = \beta(v,v) = 2x_1^2-x_2^2 + 6x_1x_2$\\\\
\textbf{\underline{Def}}:
\begin{enumerate}
    \item q è \textbf{definita positiva} se $\boldsymbol{q(v)>0}\; \forall v\neq 0$.
    \item q è \textbf{definita negativa} se -q è definita positiva, cioè se $\boldsymbol{q(v) < 0} \; \forall v \neq 0$.
    \item q è \textbf{semidefinita positiva} se $\boldsymbol{q(v) \ge 0}\;  \forall v\in V$.
    \item q è \textbf{semidefinita negativa} se $\boldsymbol{q(v) \le 0}\; \forall v \in V$.
    \item q è \textbf{indefinita} se $\exists v_1,v_2 \in V \;/\; \boldsymbol{q(v_1) >0 , q(v_2) <0}$
    
\end{enumerate}
\underline{OSS}:
\begin{enumerate}
    \item q è def pos $\iff$ la segnatura di $\beta$ è (n,0).
    Cioè $\exists$ una base in cui la matrice di $\beta$ è $I_n$. (ci sono solo 1)
    \item q è def neg $\iff $ la segatura di $\beta$ è (0,n). (ci sono solo -1)
    \item q è semidef pos $\iff$ la segnatura di $\beta$ è (r,0). (ci sono solo 1 e 0)
    \item q è semidef neg $\iff$ la segnatura di $\beta$ è (0,r). (ci sono solo -1 e 0)
    \item  q è indefinita $\iff$ la segnatura di $\beta$ è (p,r-p). (ci sono -1 e 1)
\end{enumerate}

\subsubsection{Matrice Hessiana}
   
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.6]{Hessiana.png}
    \caption{}
    \label{hessiana}
\end{figure}

\subsection{Prodotto scalare}
\textbf{\underline{Def}}: Sia V uno spazio vettoriale su $\mathbb{R}$ (si utilizza $\mathbb{R}$ perchè ci serve il concetto di ordinamento che non c'è sui complessi).\\
Un \textbf{prodotto scalare} è una forma bilineare definita positiva (cioè tale che la forma quadratica associata sia definita positiva).\\\\
\underline{OSS}: Sia $V = \mathbb{R}^n\quad v = (x_1...x_n),\quad u = (y_1...y_n)$\\
$\beta(v,u) = x_1y_1+...+x_ny_n$ è un esempio di prodotto scalare chiamato Prodotto Scalare Standard di $\mathbb{R}^n$.\\\\ Sia $\beta: V\times V \longrightarrow \mathbb{R}$ un prodotto scalare (cioè una forma bilineare simmetrica definita positiva).\\
Invece di $\beta(v,u)$ scriveremo semplicemente $\boldsymbol{(v,u)}$ (oppure $<v,u>$ o, $v\cdot u$) e invece di $q(v)$ scriveremo $||v||^2$ cioè definiamo la \textbf{norma} (o \underline{lunghezza}) di v come $\boldsymbol{||v|| = \sqrt{(v,v)}}$.\\
Osserviamo che $||v||>0 \forall v \neq \underline{0}$.\\
Diciamo che v è un \textbf{versore} se $||v|| = 1$.\\
Diciamo che $\{v_1...v_n\}$ sono vettori tra loro \textbf{ortogonali} se $(v_i,v_j) = 0 \;\forall i\neq j$.\\\\
\textbf{\underline{Def}}: Diciamo che un insieme di vettori $\{v_1...v_n\}$ è \textbf{ortonormale} se:\\
$(v_i,v_j) = \begin{cases}
    0 \quad i\neq j \;\text{ ortogonali}\\
    1 \quad i = j \;\text{ versori}
\end{cases}$

\begin{theorem}
    Per ogni spazio vettoriale V e per ogni prodotto scalare su V esiste una base ortonormale di V.
\end{theorem}
\textbf{Dim}: Poichè il prodotto scalare è def. postivo, per il teorema di Sylvester esiste una base $v_1...v_n$ in cui la matrice del prodotto scalare è In. Quindi $v_1...v_n$ sono vettori ortonormali. $\#$\\\\
Esempio: Controllo che la funzione sia un prodotto scalare.\\
Sia $V = \mathbb{R}^2\quad v = (x_1,x_2) \quad u = (y_1,y_2) \quad (v,u) = 2x_1y_1+3x_2y_2+x_1y_2+x_2y_1$.\\
Per controllare che sia un prodotto scalare deve essere 1) bilineare, 2) simmetrica, 3) def. pos.\\
Nella base $e_1, e_2$ $A=\begin{pmatrix}
    2 & 1\\
    1 & 3
\end{pmatrix}\quad$ la matrice è simmetrica, ho verificato i punti 1 e 2.\\
Diagonalizzo la matrice, così ottengo la segnatura (se è (n,0) allora è def. pos).\\
Pongo $w_1= e_1\quad w_2 =e_2-\frac{\beta(e_2,e_1)}{\beta(e_1,e_1)}e_1 = e_2-\frac{1}{2}e_1 $\\
$(w_1,w_2) = (e_1,e_2) - \frac{1}{2}(e_1,e_1) = 0$\\
$(w_2,w_2) = (e_2,e_2) + \frac{1}{4}(e_1,e_1)- \frac{2}{2}(e_1,e_2) = 3+\frac{2}{4}-1 = \frac{5}{2}$\\
La matrice in base $w_1, w_2$ diventa $\begin{pmatrix}
    2 & 0 \\
    0 & \frac{5}{2}
\end{pmatrix}$\\
Pongo $v_1 = \frac{\sqrt{2}}{2}w_1\quad v_2 = \frac{\sqrt{10}}{5}w_2\quad (v_1,v_1) = (\frac{\sqrt{2}}{2})^2(w_1,w_1) = 1 \quad (v_2,v_2) = 1\quad (v_1,v_2) = 0$\\
La matrice in base $v_1,v_2$ è $A=\begin{pmatrix}
    1 & 0\\
    0 & 1
\end{pmatrix} \Rightarrow$ def. pos\\
Quindi è un prodotto scalare e $v_1,v_2$ è una base ortonormale.
\begin{theorem}
    Sia $v_1...v_m$ un insieme di vettori ortogonali. Allora $v_1...v_m$ sono linearmente indipendenti.
\end{theorem}
\textbf{Dim}: Siano $a_1...a_m\in \mathbb{R}\;/\; a_1v_1+...+a_mv_m = \underline{0}$, dobbiamo mostrare che $a_1=0...a_m=0$.\\
$0 = (v_1, \underline{0}) = (v_1, a_1v_1+...+a_mv_m) = a_1(v_1,v_1) + a_2\underbrace{(v_1,v_2)}_{0}+...+a_m\underbrace{(v_1,v_m)}_{0}$\\
$0= a_1(v_1,v_1) \Rightarrow a_1 = 0$. Nello stesso modo verifico che $a_2 = 0...a_m = 0\;\#$.\\\\
\underline{OSS}: In particolare se n = dimV, n vettori tra loro ortogonali sono automaticamente una base.\\\\
\underline{OSS}: Un qualsiasi prod. scalare posso ricondurlo al prodotto scalare standard.\\
Sia $V$ uno spazio vettoriale su $\mathbb{R}$ con un prodotto scalare e sia $v_1...v_n$ una base ortonormale (rispetto a tale prodotto). Allora il prodotto scalare di due vettori $v,u \in V$ è uguale al prodotto scalare standard delle loro coordinate nella base $v_1...v_n$.\\\\
\textbf{Dim}:
$v = a_1v_1+...+a_nv_n\quad u = b_1v_1+...+b_nv_n$\\
$(v,u) = a_1(v_1,u)+...+a_n(v_n,u) = a_1b_1(v_1,v_1)+a_1b_2(v_1,v_2)+...+a_1b_n(v_1,v_n)+...+a_nb_1(v_n,v_1)+...+a_nb_n(v_n,v_n) \Rightarrow(v,u) = a_1b_1+...+a_nb_n$ (perchè essendo la base ortonormale se $i\neq$ j allora$(v_i,v_j) = 0) \\  
= \displaystyle\sum_{i=1}^n a_ib_i$ = prod. scalare std di $(a_1...a_n), (b_i...b_n)$
\subsection{Disuguaglianza di Cauchy-Schwartz}
\begin{theorem}
    Sia V uno spazio vettoriale su $\mathbb{R}$ con un prod. scalare. Allora $\forall v,u \in V:$
    $$\boldsymbol{(v,u)^2 \leq (v,v)\cdot (u,u)}$$
    ovvero $|(v,u)| \leq ||v||\cdot ||u||$, e l'uguaglianza vale $\iff$ v,u sono linearmente indipendenti
\end{theorem} 
\textbf{Dim}:\begin{itemize}
    \item Se NON sono lin. indip, ad es $u = av$, banalmente $(v,av)^2 = a^2(v,v)^2 = (v,v)\cdot(av,av)$
    \item Se sono lin. indip. ovvero $av+bu = \underline{0} \iff a,b = 0 $\\
    $0 < (av+bu, av+bu) =  \quad$ perchè definita positiva $(q(v) = \beta(v,v) > 0)$\\
    $= a^2(v,v) + b^2(u,u) + 2ab(v,u)$\\
    In particolare per $a = (u,u)\quad b = -(v,u)$ otteniamo:\\
    $0<(u,u)^2+(v,u)^2(u,u)-2(u,u)(v,u)^2\quad$ Dividendo per $(u,u)$ (che è $>$0):\\
    $0< (v,v)(u,u)-(v,u)^2 \Rightarrow (v,u)^2 < (v,v)(u,u)\quad \#$
    
\end{itemize}

\noindent \textbf{\underline{Def}}: Sia V uno spazio vettoriale su $\mathbb{R}$ con un prodotto scalare. Dati due vettori $v,u \in \mathbb{R}$  definiamo \textbf{l'angolo convesso} tra v,u come: 
$\boldsymbol{\theta = arccos\frac{(v,u)}{||v||\cdot||u||}}\quad$ ($\frac{(v,u)}{||v||\cdot||u||} \in
[-1,1]$ per disugu. di Shwartz).\\

\noindent \textbf{\underline{Def}}: Ogni prodotto scalare su $\mathbb{R}^n$ definisce una \textbf{distanza euclidea} nel modo seguente:\\
Dati $P,Q \in \mathbb{R}^n,\quad \boldsymbol{d(P,Q) = ||\overrightarrow{PQ}||}$.

\begin{theorem}
    La distanza euclidea verifica le seguenti proprietà:\begin{enumerate}
        \item $d(P,Q) \ge 0$ ed è = 0 $\iff$ P = Q
        \item $d(P,Q) = d(Q,P)$
        \item $d(P,Q) \leq d(P,R)+d(R,Q) \; \forall R \in \mathbb{R}^n$ (disuguaglianza triangolare)
    \end{enumerate}
\end{theorem}

\noindent \textbf{Dim}:
\begin{enumerate}
    \item Conseguenza del fatto che un prodotto scalare è def. pos.\\
    $(\overrightarrow{PQ}, \overrightarrow{PQ}) \ge 0 \quad = 0 \iff \overrightarrow{PQ} = 0 \iff P = Q$
    \item $d(Q,P) = ||\overrightarrow{QP}|| = ||-\overrightarrow{PQ}|| = |-1|\cdot||\overrightarrow{PQ}|| = d(P,Q)$\\
    Perchè più in generale, $\forall a\in \mathbb{R}, \forall v \in V:$\\
    $||av|| = \sqrt{(av,av)} = \sqrt{a^2(v,v)} = |a|\cdot ||v||$
    \item Mostriamo preliminarmente che se $v,u \in V$ allora:\\
    $||v+u||^2 = (v+u, v+u) = (v,v) + (u,u) + \underbrace{2(v,u)}_{\leq 2||v||\cdot ||u|| \;(\text{C.S.})} \leq (||v||+||u||)^2   $\\
    Quindi $||v+u|| \leq ||v|| + ||u||$\\
    sostituendo $v = \overrightarrow{PR}, u = \overrightarrow{RQ},\; v+u = \overrightarrow{PQ} \Rightarrow d(P,Q) \le d(P,R) + d(R,Q)\quad \#$
\end{enumerate}
\textbf{Esempio}:\\
Calcoliamo la distanza di $P = (1,-1) e Q = (4,3)$ rispetto al prodotto scalare standard:\\
$((x_1,x_2),(y_1,y_2)) = x_1y_1+x_2y_2$.\\
$\overrightarrow{PQ} = (4-1, 3-(-1)) = (3,4) = v$\\
$ \quad d(P,Q) = ||v|| = \sqrt{||v||} = \sqrt{x_1^2 + y_1^2} = \sqrt{3^2+4^2} = 5$\\\\
Calcoliamo la distanza euclidea degli stessi P,Q ma rispetto al prodotto scalare: \\
$((x_1,x_2),(y_1,y_2)) = 2x_1y_1+3x_2y_2+x_1y_2+x_2y_1$ che induce la norma $||v|| = \sqrt{2x_1^2+3x_2^2+2x_1x_2}$\\
Quindi $d(P,Q) = ||(3,4)|| = \sqrt{2-3^2+3*4^2+2*3*4} = 3\sqrt{10}$\\\\
\textbf{\underline{Def}}: Sia V uno spazio vettoriale su $\mathbb{R}$ con un prodotto scalare e sia U un sottospazio vettoriale di V.\\
$U^{\bot} = \{v \in V\;/\; (v,u) = 0 \forall u \in U\}\quad$ \textbf{Sottospazio ortogonale} ad U.\\\\
\underline{OSS}: $U^{\bot}$ è un SSV di V. Infatti se $v_1,v_2 \in U^{\bot}$ e $a_1,a_2 \in \mathbb{R},\;$
$a_1v_1+a_2v_2\in U^{\bot}$\\
se $u \in U,\; (a_1v_1+a_2v_2,u) = a_1(v_1,u)+a_2(v_2,u) = a_1\cdot 0+ a_2\cdot 0 = 0$\\\\
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.25]{ssv_ortogonale.png}
    \caption{Esercizio ssv ortogonale}
    \label{esercizio_ssv_ortogonale}
\end{figure}
\section{Isometrie}
\subsection{Isometria}
Sia V uno spazio vettoriale su $\mathbb{R}$ con un prodotto scalare.\\
\textbf{\underline{Def}}: Una \textbf{isometria} è una applicazione lineare $f : V \longrightarrow V$ tale che $\forall v,u \in V$:$$\boldsymbol{(f(v), f(u)) = (v,u)}$$
"Isometria" = stessa misura\\
\begin{theorem}
    f è una isometria $\iff \forall v\in V\quad ||f(v)|| = ||v||$
\end{theorem}

\noindent \textbf{Dim}:\\
$\Rightarrow$ se f è un' isometria, allora $||f(v)|| = \sqrt{(f(v), f(v))} = \sqrt{(v,v)} = ||v||$.\\\\
$\Leftarrow$ $\; \forall v,u \in V:$\\
$||v+u||^2-||v-u||^2 = (v+u, v+u)-(v-u, v-u) = (v,v)+(u,u) + 2(v,u)-[(v,v)+(u,u)-2(v,u)]$
$=4(v,u)$ cioè $(v,u) = \frac{1}{4}(||v+u||^2-||v-u||^2))$\\
Quindi se f conserva la norma dei vettori $(f(v),f(u)) = \frac{1}{4}||f(v)+f(u)||^2-||f(v)-f(u)||^2 = (v,u). \#$ \\\\
Esempio:\\
Sia $V = \mathbb{R}^2\quad v = (x,y)$ prodotto scalare standard\\
$f(x,y) = \frac{\sqrt{3}}{2}x+\frac{1}{2}y, -\frac{1}{2}x + \frac{\sqrt{3}}{2}y$\\
$||(x,y)|| = \sqrt{x^2+y^2}\quad$ perchè ($||(x,y)|| = \sqrt{(v,v)})$\\
Inserire calcoli:\\
\\\\
$||f(x,y)|| = \sqrt{x^2 + y^2} = ||(x,y)|| \Rightarrow$ f è un' isometria.\\\\
\underline{OSS}: Se f è una isometria, allora conserva gli angoli. In effetti l'angolo convesso tra $v,u$:\\
$\arccos\frac{(v,u)}{||v||||u||} = \arccos\frac{(f(v), f(u))}{||f(v)||||f(u||}\quad$ che è l'angolo compreso tra $f(v),f(u)$.\\\\
\underline{OSS}: Non tutte le applicazioni lineari che conservano gli angoli sono isometrie.\\
Ad esempio:\\
$f:\mathbb{R}^n\rightarrow \mathbb{R}^n\quad f(v) = 2v$\\
$\frac{(f(v), f(u))}{||f(v)||||f(u||} = \frac{4(v,u)}{4(||v||||u||}\quad$ f non cambia gli angoli ma sicuramente f non è una isometria perchè $||f(v)|| \neq ||v||$.\\
\begin{theorem}
    Ogni isometria è un isomorfismo
\end{theorem}
\textbf{Dim}: Sia f un isometria. Facciamo prima vedere che f è iniettiva. Poiché un prodotto scalare è def. positivo, $||v|| = 0 \iff v = \underline{0}$.\\
Sia v $\in kerf$, cioè $f(v) = \underline{0}$. Poichè f è una isometria $||v|| = ||f(v)|| = ||\underline{0}||$.\\
Quindi $kerf = \{\underline{0}\} \Rightarrow$ f è iniettiva.\\
Adesso dimostro che è suriettiva.\\
Poichè $f: V\rightarrow V$, per il teorema del rango dim Imf = dimV - dim kerf = dimV.\\
Quindi $\Rightarrow$ f è suriettiva $\Rightarrow$ f è un isomorfismo. \#\\\\

\noindent\underline{OSS}: Non vale il viceversa: ad esempio $f:V\rightarrow V\quad f(v) = 2v$ è un isomorfismo ma \textbf{non} è una isometria.\\\\
\textbf{\underline{Def}}: Una matrice A è ortogonale se $\boldsymbol{AA^T = In}$.\\\\
Esempio:\\
$\begin{pmatrix}
    \frac{\sqrt{3}}{2} & \frac{1}{2}\\
    -\frac{1}{2} & \frac{\sqrt{3}}{2}
\end{pmatrix}$ è ortogonale perchè $AA^T = \begin{pmatrix}
    \frac{\sqrt{3}}{2} & \frac{1}{2}\\
    -\frac{1}{2} & \frac{\sqrt{3}}{2}
\end{pmatrix} \begin{pmatrix}
    \frac{\sqrt{3}}{2} & -\frac{1}{2}\\
    \frac{1}{2} & \frac{\sqrt{3}}{2}
\end{pmatrix} = \begin{pmatrix}
    1 & 0\\
    0 & 1
\end{pmatrix} $\\\\
\underline{OSS}: Questo ci fa sospettare che ci sia un legame tra matrici ortogonali e isometrie.\\
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.15]{mappa.jpeg}
    \caption{Collegamento teoremi}
    \label{mappa_teoremi}
\end{figure}
\subsection{Teoremi isometrie e matrici ortogonali}
\begin{theorem}[T.1]
Sia $v_1...v_n$ una base ortonormale di V. Allora una base $w_1...w_n$ di V è anch'essa ortonormale $\iff$ la matrice del cambiamento di base è ortogonale.
\end{theorem}
\textbf{Dim}: Poichè $v_1...v_n$ è ortonormale, cioè $(v_1...v_j) = \begin{cases}
    1 \;se\; i=j\\
    0 \; se\; i\neq j
\end{cases}$ la matrice del prodotto scalare in tale base è In. Quindi, se $C$ è la matrice del cambiamento di base, la matrice del prodotto scalare nella base $w_1...w_n$ è $C^TInC = C^TC$ (uso In perchè deve essere congruente ad una matrice identità). Quindi $w_1...w_n$ è ortonormale $\iff$ la matrice del prodotto scalare in tale base è In $\iff C^TC = In \iff$ C è ortogonale. $\#$\\\\
In precedenza abbiamo visto che un'applicazione lineare è un isomorfismo $\iff$ manda basi in basi.\\
\begin{theorem}[T.2]
Un' applicazione lineare è una isometria $\iff$ manda basi ortonormali in basi ortonormali
\end{theorem}
\textbf{Dim}:\\
$\Rightarrow$ Sia f una isometria e $v_1...v_n$ una base ortonormale. Allora:\\
$(f(v_i), f(v_j)) = (v_i,v_j) = \begin{cases}
    1\; se\; i = j\\
    0\; se \; i\neq j
\end{cases}$ cioè $f(v_1)...f(v_n)$ è una base ortonormale.\\
$\Leftarrow$ Sia f un'applicazione lineare e sia $v_1...v_n$ una base ortonormale tale che $f(v_1)...f(v_n)$ è una base ortonormale.\\
Voglio mostrare che f è una isometria. Dati $v,u$, li scrivo nella base $v_1...v_n$:\\
$v = x_1v_1+...+x_nv_n\quad u = y_1v_1+...+y_nv_n$\\
$f(v) = x_1f(v_1)+...+x_nf(v_n)$, $f(u) = y_1f(v_1)+...+y_nf(v_n)$\\
Quindi, per il teorema "il prodotto scalare di due vettori v,u è uguale al prod. scalare std delle loro coordinate in una base ortonormale":\\
$(v,u) = x_1y_1+...+x_ny_n$ (perchè essendo la base ortonormale se $i\neq j$
allora$(v_i, v_j ) = 0$) = $(f(v), f(u))$(perchè anche $f(v_1)...f(v_n)$ è ortonormale).
Quindi f è una isometria. $\quad \#$\\
\begin{theorem}[T.3]
Un' applicazione lineare f è una isometria $\iff$ la sua matrice in una qualsiasi base ortonormale è una matrice ortogonale.
\end{theorem}
\textbf{Dim}: Sia $v_1...v_n$ una base ortonormale e sia A la matrice di f rispetto a tale base\\
$f(v_1) = a_{11}v_1+...+a_{n1}v_n$\\
...\\
$f(v_n) = a_{1n}v_1+...+a_{nn}\quad \quad \quad A = \begin{pmatrix}
    a_{11} & ... & a_{1n}\\
    ... & ... & ...\\
    a_{n1} & ... & a_{nn}
\end{pmatrix}$\\
$(f(v_i),f(v_j)) = (a_{1i}v_1+...+a_{ni}v_n, a_{1j}v_i+...+a_{nj}v_n)=$\\
(utilizzo la bilinearità del prodotto scalare e il fatto che $v_1...v_n$ sono ortonormali)\\
$= a_{1i}a_{1j}(v_1,v_1)+...+a_{ni}a_{nj}(v_n,v_n)$\\
Cioè $(f(v_i), f(v_j)) = a_{1i}a_{1j}+...+a_{ni}a_{nj} = (A^TA)_{ij}$\\
Infatti $A^TA = \begin{pmatrix}
    a_{11} & ... & a_{1n}\\
    a_{1i} & ... & a_{ni}\\
    a_{in} & ... & a_{nn}
\end{pmatrix}\begin{pmatrix}
    a_{11} & a_{1j} & a_{in}\\
    ... & ... & ...\\
    a_{n1} & a_{nj} & a_{nn}
\end{pmatrix}$\\
Quindi i-ma riga di A per j-ma colonna di $A^T$ è proprio la somma qui sopra.\\
Quindi f è un isometria $\iff (f(v_i),f(v_j)) = \begin{cases}
    1 \;se\; i = j\\
    0 \;se\; i\neq j
\end{cases}$ (per il T.2) $\iff (A^TA)_{ij} = \begin{cases}
    1\;se\; i=j\\
    0\;se\; i\neq j
\end{cases} \iff A^TA = In \iff $ A è una matrice ortogonale $\#$.\\\\
Esempio:\\\\\\\\\

\subsection{Altre proprietà}
\begin{theorem}
    \begin{enumerate}
        \item Sia A una matrice ortogonale. Allora $det(A) = \pm 1$
        \item Sia f un' isometria. Allora det(f) = $\pm 1$
    \end{enumerate}
\end{theorem}

\noindent\textbf{Dim}:\\
1) det($A^T)$ = det(A). Quindi se $A^TA = In$ allora det($A^TA) = $ det(A)det($A^T) = $ (det(A))$^2$
= det(In) = 1 $\iff A = \pm1 $\\
2) Per definizione, det(f) = det(A) dove A è la matrice di f in una base qualsiasi. Se scelgo una base ortonormale, per il T.3 la matrice A di f è ortogonale, e quindi utilizzo quanto visto al punto 1). $\#$\\\\
\underline{OSS}: Non è vero il viceversa. Ad esempio $A = \begin{pmatrix}
    2 & 0\\
    0 & \frac{1}{2}
\end{pmatrix}\quad$ det(A) = 1 ma A non è ortogonale.
Perchè $A^TA = \begin{pmatrix}
    4 & 0 \\
    0 & \frac{1}{4}
\end{pmatrix}$ e l'applicazione $f(x,y) = (2x, \frac{1}{2}y)$ non è una isometria.
\begin{theorem}
    Sia f una isometria. Allora se $\lambda \in \mathbb{R}$ è un autovalore reale di f, allora $\lambda = \pm 1$
\end{theorem}
\textbf{Dim}: Se v è un autovettore di f, cioè $\exists v \neq \underline{0} \;/\; f(v) = \lambda v$, allora (isometria) $||f(v)|| = ||\lambda v|| = \sqrt{(\lambda v, \lambda v)} = \sqrt{\lambda ^ 2(v,v)} = \sqrt{\lambda^2}\sqrt{(v,v)} = |\lambda|\cdot||v||$.\\
Poichè f è una isometria $||v|| = ||f(v)|| = |\lambda|||v||$ quindi $|\lambda| = 1 \Rightarrow \lambda = \pm 1. \#$\\\\
\underline{OSS}: il teorema dice che \underline{se} $\lambda$ è reale allora è $\pm 1$. Ma può essere anche complesso.\\
Esempio:\\
$f(x,y) = (y, -x)$ nella base canonica $\begin{pmatrix}
    0 & 1\\
    -1 & 0
\end{pmatrix}$\\
$p(\lambda) = det\begin{pmatrix}
    -\lambda & 1\\
    -1 & -\lambda
\end{pmatrix} = \lambda ^ 2+1 \Rightarrow \lambda_1 = i\; \lambda_2 = -i$\\
quindo f è una isometria i cui autovalori sono $i,-i$.



\end{document}